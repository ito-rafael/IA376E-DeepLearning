\BOOKMARK [1][-]{section.1}{Sentiment analysis using simplified self-attention}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{0. Dataset and Description}{section.1}% 2
\BOOKMARK [2][-]{subsection.1.2}{1. Libraries and packages}{section.1}% 3
\BOOKMARK [3][-]{subsubsection.1.2.1}{1.1 Install packages}{subsection.1.2}% 4
\BOOKMARK [3][-]{subsubsection.1.2.2}{1.2 Import libraries}{subsection.1.2}% 5
\BOOKMARK [3][-]{subsubsection.1.2.3}{1.3 Check device}{subsection.1.2}% 6
\BOOKMARK [3][-]{subsubsection.1.2.4}{1.4 Constants definition}{subsection.1.2}% 7
\BOOKMARK [2][-]{subsection.1.3}{2. Custom functions and classes}{section.1}% 8
\BOOKMARK [3][-]{subsubsection.1.3.1}{2.1 Functions}{subsection.1.3}% 9
\BOOKMARK [3][-]{subsubsection.1.3.2}{2.2 Classes}{subsection.1.3}% 10
\BOOKMARK [2][-]{subsection.1.4}{3. Dataset Pre-processing}{section.1}% 11
\BOOKMARK [3][-]{subsubsection.1.4.1}{3.1 Download dataset}{subsection.1.4}% 12
\BOOKMARK [3][-]{subsubsection.1.4.2}{3.2 Download embeddings}{subsection.1.4}% 13
\BOOKMARK [3][-]{subsubsection.1.4.3}{3.3 Dataset preparation}{subsection.1.4}% 14
\BOOKMARK [2][-]{subsection.1.5}{Self-Attention Layer \(as function\)}{section.1}% 15
\BOOKMARK [3][-]{subsubsection.1.5.1}{Self-Attention using the Loop Approach}{subsection.1.5}% 16
\BOOKMARK [3][-]{subsubsection.1.5.2}{Self-Attention using the Matrix Approach}{subsection.1.5}% 17
\BOOKMARK [2][-]{subsection.1.6}{Dataset}{section.1}% 18
\BOOKMARK [3][-]{subsubsection.1.6.1}{3.3 PyTorch dataset creation}{subsection.1.6}% 19
\BOOKMARK [3][-]{subsubsection.1.6.2}{3.4 PyTorch loader creation}{subsection.1.6}% 20
\BOOKMARK [3][-]{subsubsection.1.6.3}{3.5 Verifying shape, batch data type from loader and optionally its visualization}{subsection.1.6}% 21
\BOOKMARK [2][-]{subsection.1.7}{4. Network Model}{section.1}% 22
\BOOKMARK [3][-]{subsubsection.1.7.1}{4.1 Network class definition}{subsection.1.7}% 23
\BOOKMARK [3][-]{subsubsection.1.7.2}{4.2 Network instantiation}{subsection.1.7}% 24
\BOOKMARK [3][-]{subsubsection.1.7.3}{4.3 Network predict with few samples of batch from loader}{subsection.1.7}% 25
\BOOKMARK [2][-]{subsection.1.8}{5. Network training}{section.1}% 26
\BOOKMARK [3][-]{subsubsection.1.8.1}{5.1 Training definitions}{subsection.1.8}% 27
\BOOKMARK [3][-]{subsubsection.1.8.2}{5.2 Training loop}{subsection.1.8}% 28
\BOOKMARK [2][-]{subsection.1.9}{6. Training evaluation}{section.1}% 29
\BOOKMARK [3][-]{subsubsection.1.9.1}{6.1 Training and Validation Losses}{subsection.1.9}% 30
\BOOKMARK [3][-]{subsubsection.1.9.2}{6.2 Zoom at the minimum CE loss in the validation loss curve}{subsection.1.9}% 31
\BOOKMARK [3][-]{subsubsection.1.9.3}{6.3 Accuracy}{subsection.1.9}% 32
\BOOKMARK [3][-]{subsubsection.1.9.4}{6.4 Print the final values of the main training monitoring variables:}{subsection.1.9}% 33
\BOOKMARK [2][-]{subsection.1.10}{7. Metrics on test set}{section.1}% 34
\BOOKMARK [3][-]{subsubsection.1.10.1}{7.1 Accuracy}{subsection.1.10}% 35
\BOOKMARK [3][-]{subsubsection.1.10.2}{7.2 Confusion matrix}{subsection.1.10}% 36
\BOOKMARK [3][-]{subsubsection.1.10.3}{7.3 F1-score \(macro, micro and weighted\)}{subsection.1.10}% 37
\BOOKMARK [3][-]{subsubsection.1.10.4}{7.4 Accuracy and Precision}{subsection.1.10}% 38
\BOOKMARK [3][-]{subsubsection.1.10.5}{7.5 Precision, Recall and F1-Score for each class}{subsection.1.10}% 39
\BOOKMARK [2][-]{subsection.1.11}{8. Number of parameters}{section.1}% 40
\BOOKMARK [2][-]{subsection.1.12}{End of the notebook}{section.1}% 41
