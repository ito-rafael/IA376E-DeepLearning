{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[week 3] T5 step-by-step.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oVBNMCzfl4nA"
      },
      "source": [
        "# PF06 - NSGC: Neural Spell & Grammar Checker (en/pt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S5EtRG_8l4nK"
      },
      "source": [
        "Author: **Rafael Ito**  \n",
        "e-mail: ito.rafael@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IlQelJYEl4nP"
      },
      "source": [
        "# 0. Dataset and Description\n",
        "\n",
        "**Name:**  CoNLL-2014, JFLEG, BEA  \n",
        "**Description:** in this notebook we will use BERT and T5 to predict words in a sentence to perform a spell and grammar checker for Portuguese and English languages. For English, we will use the BERT and T5 models from transformers library (huggingface) and evaluate the performance in CoNLL-2014 and JFLEG datasets. For Portuguese, we will use the transformers/neuralmind BERT version and a custom dataset for evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cl1mzsXxl4nR"
      },
      "source": [
        "# 1. Libraries and packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ygmMkUpUl4nV"
      },
      "source": [
        "## 1.1 Check device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3BUcojudl4nY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "44d21711-b4af-4d5f-ffa3-5c5093abf2b3"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    device_model = torch.cuda.get_device_name(0)\n",
        "print('GPU model:', device_model)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU model: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K38L1dY2l4nl"
      },
      "source": [
        "## 1.2 Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5K7PPVNel4nn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "9efea591-0ba8-47a4-bef5-dfbf8fe794e8"
      },
      "source": [
        "# install Python libs\n",
        "!pip install -q     \\\n",
        "    numpy           \\\n",
        "    torch           \\\n",
        "    transformers    \n",
        "#----------------------------\n",
        "# install PyEnchant\n",
        "! apt-get -qq update\n",
        "! apt-get -qq install libenchant-dev\n",
        "! pip install -q pyenchant\n",
        "#----------------------------\n",
        "# string similarity and distance\n",
        "! pip install strsimpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: strsimpy in /usr/local/lib/python3.6/dist-packages (0.1.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VuYuF9lQl4nv"
      },
      "source": [
        "## 1.3 Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pdzUjbSel4nx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "9dc40429-9896-4750-eb36-bb0dc6de55a5"
      },
      "source": [
        "#-------------------------------------------------\n",
        "# general\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "import pdb\n",
        "import codecs\n",
        "import subprocess\n",
        "from multiprocessing import cpu_count\n",
        "#-------------------------------------------------\n",
        "# NLP\n",
        "from transformers import T5Tokenizer, BertTokenizer, BertForMaskedLM, T5ForConditionalGeneration\n",
        "import enchant\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words\n",
        "#-------------------------------------------------\n",
        "# Edit distance algorithms\n",
        "from strsimpy.levenshtein import Levenshtein\n",
        "from strsimpy.normalized_levenshtein import NormalizedLevenshtein\n",
        "from strsimpy.weighted_levenshtein import WeightedLevenshtein\n",
        "from strsimpy.weighted_levenshtein import CharacterSubstitutionInterface\n",
        "from strsimpy.damerau import Damerau\n",
        "from strsimpy.optimal_string_alignment import OptimalStringAlignment\n",
        "#-------------------------------------------------\n",
        "# random seed generator\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "#-------------------------------------------------\n",
        "# Suppress some of the logging\n",
        "import logging\n",
        "logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.WARNING)\n",
        "#-------------------------------------------------\n",
        "# Suppress warning messages\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "#-------------------------------------------------\n",
        "# package version\n",
        "print('Torch version:', torch.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "Torch version: 1.5.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vPl_z4WVl4n5"
      },
      "source": [
        "## 1.4 Device info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YnKjGG5Bl4n7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "outputId": "292ef705-fe9c-44e9-d03f-6eae7abc9c9b"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    device_model = torch.cuda.get_device_name(0)\n",
        "    device_memory = torch.cuda.get_device_properties(device).total_memory / 1e9\n",
        "#----------------------------\n",
        "print('Device:', device)\n",
        "print('GPU model:', device_model)\n",
        "print('GPU memory: {0:.2f} GB'.format(device_memory))\n",
        "print('#-------------------')\n",
        "print('CPU cores:', cpu_count())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "GPU model: Tesla P100-PCIE-16GB\n",
            "GPU memory: 17.07 GB\n",
            "#-------------------\n",
            "CPU cores: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bxONI8xdl4oD"
      },
      "source": [
        "# 2. Custom functions and classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrRWKsWkAkdU",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Function to read file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rrctdeQRl4oH",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function that reads a file and return its text\n",
        "#------------------------------------------------------\n",
        "parameters:\n",
        "    - path: path of the file to be read\n",
        "    - encoding: encoding to be used\n",
        "returns:\n",
        "    file content as list of strings\n",
        "'''\n",
        "def read_file(path, encoding='utf-8'):\n",
        "    with codecs.open(path, encoding=encoding) as f:\n",
        "        return f.read().splitlines()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpyM_L_ly9Yc",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Function to write in file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLCa0dLi0tqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function that writes list of strings in a file\n",
        "#------------------------------------------------------\n",
        "parameters:\n",
        "    - sentences: list of strings to be written in file\n",
        "    - path: path of the file where strings will be written\n",
        "returns:\n",
        "    path: same as input\n",
        "'''\n",
        "def write_file(sentences, path, encoding='utf-8'):\n",
        "    with codecs.open(path, 'w', encoding=encoding) as f:\n",
        "        for sentence in sentences:\n",
        "            f.write(sentence + '\\n')\n",
        "    f.close()\n",
        "    return path"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l5phhds3aCYP"
      },
      "source": [
        "## 2.3 Function to get tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qJOiUWH4aCYQ",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function that returns the tokenizer associated to a string\n",
        "#------------------------------------------------------\n",
        "parameters:\n",
        "    tokenizer:\n",
        "      BERT options:\n",
        "        - 'bert-base-cased'\n",
        "        - 'bert-large-cased'\n",
        "        - 'bert-base-uncased'\n",
        "        - 'bert-large-uncased'\n",
        "      T5 options:\n",
        "        - 't5-small'\n",
        "        - 't5-base'\n",
        "        - 't5-large'\n",
        "        - 't5-3b'\n",
        "        - 't5-11b'\n",
        "      otherwise raise an error\n",
        "returns:\n",
        "    Hugging Face's tokenizer\n",
        "'''\n",
        "def get_tokenizer(tokenizer):\n",
        "    # BERT\n",
        "    if ((tokenizer == 'bert-base-cased') or \n",
        "        (tokenizer == 'bert-large-cased') or\n",
        "        (tokenizer == 'bert-base-uncased') or \n",
        "        (tokenizer == 'bert-large-uncased') or\n",
        "        (tokenizer == 'neuralmind/bert-large-portuguese-cased') or \n",
        "        (tokenizer == 'neuralmind/bert-base-portuguese-cased')):\n",
        "        return BertTokenizer.from_pretrained(tokenizer)\n",
        "    #------------------------------------------------------\n",
        "    # T5\n",
        "    elif ((tokenizer == 't5-small') or \n",
        "          (tokenizer == 't5-base') or \n",
        "          (tokenizer == 't5-large') or \n",
        "          (tokenizer == 't5-3b') or \n",
        "          (tokenizer == 't5-11b')):\n",
        "        return T5Tokenizer.from_pretrained(tokenizer)\n",
        "    #------------------------------------------------------\n",
        "    else:\n",
        "        raise ValueError(f'Unsupported tokenizer: {tokenizer}')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f6tJ9-RCl4of"
      },
      "source": [
        "## 2.4 Function to get model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Qepgfyrl4og",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function that returns the the network model associated to a string\n",
        "#------------------------------------------------------\n",
        "parameters:\n",
        "    model_name:\n",
        "      BERT models:\n",
        "        - 'bert-base-cased'                         # BERT base  cased   [en] (110 M params)\n",
        "        - 'bert-large-cased'                        # BERT large cased   [en] (340 M params)\n",
        "        - 'bert-base-uncased'                       # BERT base  uncased [en] (110 M params)\n",
        "        - 'bert-large-uncased'                      # BERT large uncased [en] (340 M params)\n",
        "        - 'neuralmind/bert-base-portuguese-cased'   # BERT base  cased   [pt] (110 M params)\n",
        "        - 'neuralmind/bert-large-portuguese-cased'  # BERT large cased   [pt] (340 M params)\n",
        "      T5 models:\n",
        "        - 't5-small' (60 M params)\n",
        "        - 't5-base'  (220 M params)\n",
        "        - 't5-large' (770 M params)\n",
        "        - 't5-3B'    (2.8 B params)\n",
        "        - 't5-11B'   (11 B params)\n",
        "      otherwise raise an error\n",
        "returns:\n",
        "    Hugging Face's model\n",
        "'''\n",
        "def get_model(model_name):\n",
        "    # BERT\n",
        "    if ((model_name == 'bert-base-cased') or                        # BERT base  cased   [en]\n",
        "        (model_name == 'bert-large-cased') or                       # BERT large cased   [en]\n",
        "        (model_name == 'bert-base-uncased') or                      # BERT base  uncased [en]\n",
        "        (model_name == 'bert-large-uncased') or                     # BERT large uncased [en]\n",
        "        (model_name == 'neuralmind/bert-base-portuguese-cased') or  # BERT base  cased   [pt]\n",
        "        (model_name == 'neuralmind/bert-large-portuguese-cased')):  # BERT large cased   [pt]\n",
        "        return BertForMaskedLM.from_pretrained(model_name)\n",
        "    #------------------------------------------------------\n",
        "    # T5\n",
        "    elif ((model_name == 't5-small') or     # T5 small [en]   242 MB\n",
        "          (model_name == 't5-base')  or     # T5 base  [en]   892 MB\n",
        "          (model_name == 't5-large') or     # T5 large [en]  2.95 GB\n",
        "          (model_name == 't5-3B')    or     # T5 3B    [en]  11.4 GB\n",
        "          (model_name == 't5-11B')):        # T5 11B   [en]  ??.? GB\n",
        "        return T5ForConditionalGeneration.from_pretrained(model_name, use_bfloat16=True)\n",
        "    #------------------------------------------------------\n",
        "    else:\n",
        "        raise ValueError(f'Unsupported model: {model_name}')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c93U-7ppa9Z",
        "colab_type": "text"
      },
      "source": [
        "## 2.5 Function to edit distance algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI2HyauqlhLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function that returns the algorithm to calculate the edit distance\n",
        "#------------------------------------------------------\n",
        "parameters:                   +--------------------------+---------+\n",
        "    algorithm:                |        algorithm         | metric? |\n",
        "                              +--------------------------+---------+\n",
        "        - 'levenshtein'       | Levenshtein              |   yes   |\n",
        "        - 'normalized'        | Normalized Levenshtein   |   no    |\n",
        "        - 'weighted'          | Weighted Levenshtein     |   no    |\n",
        "        - 'damerau'           | Damerau-Levenshtein      |   yes   |\n",
        "        - 'osa'               | Optimal String Alignment |   no    |\n",
        "    otherwise raise an error  +--------------------------+---------+\n",
        "returns:\n",
        "    edit distance algorithm\n",
        "'''\n",
        "def get_distance_algorithm(algorithm):\n",
        "    if (algorithm == 'levenshtein'):\n",
        "        return Levenshtein()\n",
        "    elif (algorithm == 'normalized'):\n",
        "        return NormalizedLevenshtein()\n",
        "    elif (algorithm == 'weighted'):\n",
        "        return \n",
        "    elif (algorithm == 'damerau'):\n",
        "        return Damerau()\n",
        "    elif (algorithm == 'osa'):\n",
        "        return OptimalStringAlignment()\n",
        "    else:\n",
        "        raise ValueError(f'Unsupported algorithm: {algorithm}')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6YO8zkSPzWe",
        "colab_type": "text"
      },
      "source": [
        "## 2.6 Function to calculate GLEU score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raq4l8OAdXfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function that receives text files and calculate GLEU score\n",
        "#------------------------------------------------------\n",
        "parameters:\n",
        "    - src: source file\n",
        "    - ref: reference file(s)\n",
        "    - hyp: hypothesis file\n",
        "    - n: n-gram order\n",
        "    - num_iter: number of GLEU iterations\n",
        "    - sent: sentence level scores\n",
        "returns:\n",
        "    GLEU score (float)\n",
        "'''\n",
        "def calc_gleu(src, ref, hyp, n=4, num_iter=500, sent=False):\n",
        "    gleu_calculator.load_sources(src)\n",
        "    gleu_calculator.load_references(ref)\n",
        "    if len(ref) == 1:\n",
        "        print(\"There is one reference. NOTE: GLEU is not computing the confidence interval.\")\n",
        "        gleu = [g for g in gleu_calculator.run_iterations(\n",
        "            num_iterations=num_iter,\n",
        "            source=src,\n",
        "            hypothesis=hyp,\n",
        "            per_sent=sent)][0][0] \n",
        "    else:\n",
        "        gleu = [g for g in gleu_calculator.run_iterations(\n",
        "            num_iterations=num_iter,\n",
        "            source=src,\n",
        "            hypothesis=hyp,\n",
        "            per_sent=sent)][0][0]\n",
        "    #print(gleu)\n",
        "    return float(gleu)*100"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K85K64M_7Vo2",
        "colab_type": "text"
      },
      "source": [
        "## 2.7 Function to calculate MaxMatch score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ-tjeb-5JW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function that runs Python 2 script to calculate M^2 score\n",
        "#------------------------------------------------------\n",
        "parameters:\n",
        "    - src_file_path: source file path\n",
        "    - ref_file_path: reference file path\n",
        "returns:\n",
        "    MaxMatch score (precision, recall, F_{0.5}) as string\n",
        "'''\n",
        "def m2scorer(src_file_path, ref_file_path):\n",
        "    process = subprocess.Popen(['/content/m2scorer/scripts/m2scorer.py', src_file_path, ref_file_path], stdout=subprocess.PIPE)\n",
        "    output, error = process.communicate()\n",
        "    output = output.decode(\"utf-8\")\n",
        "    return output"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c97AaIugm5CT",
        "colab_type": "text"
      },
      "source": [
        "## 2.8 Function parse M2 file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McGSJNNafjDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function that receives M2 format file and returns original sentences\n",
        "#------------------------------------------------------\n",
        "parameters:\n",
        "    - m2_file: reference file in M2 format\n",
        "    - output_file: file where the output will be written\n",
        "returns:\n",
        "    list of strings with original sentences\n",
        "'''\n",
        "def m2_parser(m2_file, output_file):\n",
        "    # create output file\n",
        "    !touch /content/conll14st-test-data/noalt/official-2014.1.cor\n",
        "    # delete annotations, blank lines and 'S ' at the beginning of sentences\n",
        "    !sed -e '/^A/d' -e '/^$/d' -e 's/^S //g' $m2_file > $output_file\n",
        "    # read output file and return it as list of string\n",
        "    conll_2014_test_src = read_file(output_file)\n",
        "    return conll_2014_test_src"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AM1FKq-_l4pL"
      },
      "source": [
        "# 3. Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UWXy8iYIbPGQ"
      },
      "source": [
        "## 3.1 CoNLL-2013"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HPQMtQKtbPGW"
      },
      "source": [
        "### 3.1.1 Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l9ZWF4GMbPGb",
        "colab": {}
      },
      "source": [
        "# test set\n",
        "! wget -q -nc https://www.comp.nus.edu.sg/~nlp/conll13st/release2.3.1.tar.gz\n",
        "! tar -xzf release2.3.1.tar.gz\n",
        "! rm release2.3.1.tar.gz"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-rN8qr49bPG3"
      },
      "source": [
        "### 3.1.2 Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vLxelb0BbPG4",
        "colab": {}
      },
      "source": [
        "# import test set\n",
        "#---------------------------\n",
        "# source\n",
        "m2_file     = '/content/release2.3.1/revised/data/official-preprocessed.m2'\n",
        "output_file = '/content/release2.3.1/revised/data/official-preprocessed.src'\n",
        "conll_2013_test_src = m2_parser(m2_file, output_file)\n",
        "# reference\n",
        "conll_2013_test_ref = read_file(m2_file)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hpRqjctlbPHA"
      },
      "source": [
        "### 3.1.3 Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y8nkpoxDbPHB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "520ffaa8-0a0a-4ccb-a53a-eb9f4a3e7081"
      },
      "source": [
        "print('original sentence:')\n",
        "print(conll_2013_test_src[0])\n",
        "#---------------------------\n",
        "print('\\nannotation:')\n",
        "print(*conll_2013_test_ref[0:4], sep='\\n')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original sentence:\n",
            "In modern digital world , electronic products are widely used in daily lives such as Smart phones , computers and etc .\n",
            "\n",
            "annotation:\n",
            "S In modern digital world , electronic products are widely used in daily lives such as Smart phones , computers and etc .\n",
            "A 1 1|||ArtOrDet|||the|||REQUIRED|||-NONE-|||0\n",
            "A 12 13|||Nn|||life|||REQUIRED|||-NONE-|||0\n",
            "A 15 16|||Mec|||smart|||REQUIRED|||-NONE-|||0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7gtwXk9ul4ph"
      },
      "source": [
        "## 3.2 CoNLL-2014"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmtSxziZOKnO",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.1 Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70DO7E2GOMsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## training set\n",
        "#from google.colab import drive\n",
        "#drive.mount('/gdrive')\n",
        "#---------------------------\n",
        "# test set\n",
        "! wget -q -nc https://www.comp.nus.edu.sg/~nlp/conll14st/conll14st-test-data.tar.gz\n",
        "! tar -xzf conll14st-test-data.tar.gz\n",
        "! rm conll14st-test-data.tar.gz"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5gY1fUrBQTM",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.2 Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sSaSMppLl4po",
        "colab": {}
      },
      "source": [
        "# # import training set\n",
        "# #---------------------------\n",
        "# source\n",
        "# m2_file     = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/CoNLL-2014/release3.3/data/conll14st-preprocessed.m2'\n",
        "# output_file = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/CoNLL-2014/release3.3/data/conll14st-preprocessed.src'\n",
        "# conll_2014_test_src = m2_parser(m2_file, output_file)\n",
        "# # reference\n",
        "# conll_2014_train_ref = read_file(m2_file)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48DvLgUL-YWQ",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.3 Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PklOfNoe-aBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import test set\n",
        "#---------------------------\n",
        "# source\n",
        "m2_file     = '/content/conll14st-test-data/noalt/official-2014.1.m2'\n",
        "output_file = '/content/conll14st-test-data/noalt/official-2014.1.src'\n",
        "conll_2014_test_src = m2_parser(m2_file, output_file)\n",
        "# reference\n",
        "conll_2014_test_ref = read_file(m2_file)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND1owATCBSVv",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.4 Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LDPER-iJwC3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "be0c8635-8a6e-4bb9-f5ba-4063ceecd8c9"
      },
      "source": [
        "print('original sentence:')\n",
        "print(conll_2014_test_src[3])\n",
        "#---------------------------\n",
        "print('\\nannotation:')\n",
        "print(*conll_2014_test_ref[7:9], sep='\\n')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original sentence:\n",
            "People get certain disease because of genetic changes .\n",
            "\n",
            "annotation:\n",
            "S People get certain disease because of genetic changes .\n",
            "A 3 4|||Nn|||diseases|||REQUIRED|||-NONE-|||0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Id_41Snkl4p2"
      },
      "source": [
        "## 3.3 JFLEG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3c37lULNtt2",
        "colab_type": "text"
      },
      "source": [
        "### 3.3.1 Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhChJtUzNvYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clone GitHub repo\n",
        "! git clone --quiet https://github.com/keisks/jfleg.git 2> /dev/null"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_j57PTqKiPX",
        "colab_type": "text"
      },
      "source": [
        "### 3.3.2 Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_TLpCnYLl4p9",
        "colab": {}
      },
      "source": [
        "# import training set\n",
        "#---------------------------\n",
        "# source\n",
        "jfleg_train_src = read_file('jfleg/dev/dev.src')\n",
        "# references\n",
        "jfleg_train_ref0 = read_file('jfleg/dev/dev.ref0')\n",
        "jfleg_train_ref1 = read_file('jfleg/dev/dev.ref1')\n",
        "jfleg_train_ref2 = read_file('jfleg/dev/dev.ref2')\n",
        "jfleg_train_ref3 = read_file('jfleg/dev/dev.ref3')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZL0W3y5CNcex"
      },
      "source": [
        "### 3.3.3 Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3sqG-fWFNce3",
        "colab": {}
      },
      "source": [
        "# import test set\n",
        "#---------------------------\n",
        "# source\n",
        "jfleg_test_src = read_file('jfleg/test/test.src')\n",
        "# references\n",
        "jfleg_test_ref0 = read_file('jfleg/test/test.ref0')\n",
        "jfleg_test_ref1 = read_file('jfleg/test/test.ref1')\n",
        "jfleg_test_ref2 = read_file('jfleg/test/test.ref2')\n",
        "jfleg_test_ref3 = read_file('jfleg/test/test.ref3')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUNVKEuFKoXH",
        "colab_type": "text"
      },
      "source": [
        "### 3.3.4 Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ziDQ_GYQl4qH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "f44f3e6f-07e1-478a-9abe-d8c577ea81c4"
      },
      "source": [
        "# print source and references example\n",
        "print('source sentence:')\n",
        "print(jfleg_test_src[0])\n",
        "#---------------------------\n",
        "print('\\nreferences sentences:')\n",
        "print(jfleg_test_ref0[0])\n",
        "print(jfleg_test_ref1[0])\n",
        "print(jfleg_test_ref2[0])\n",
        "print(jfleg_test_ref3[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source sentence:\n",
            "New and new technology has been introduced to the society .\n",
            "\n",
            "references sentences:\n",
            "New technology has been introduced to society .\n",
            "New technology has been introduced into the society .\n",
            "Newer and newer technology has been introduced into society .\n",
            "Newer and newer technology has been introduced to the society .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sn5WKnjHl4qL"
      },
      "source": [
        "## 3.4 BEA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbbURbluKq-b",
        "colab_type": "text"
      },
      "source": [
        "### 3.4.1 Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a-1bHtPhl4qN",
        "colab": {}
      },
      "source": [
        "# download test data\n",
        "! wget -q -nc https://www.cl.cam.ac.uk/research/nl/bea2019st/data/wi+locness_v2.1.bea19.tar.gz\n",
        "! tar -xzf wi+locness_v2.1.bea19.tar.gz\n",
        "! rm wi+locness_v2.1.bea19.tar.gz"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CLbTX4UWLd3c"
      },
      "source": [
        "### 3.4.2 Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lShaa-b4Ld3m",
        "colab": {}
      },
      "source": [
        "# import test set\n",
        "#---------------------------\n",
        "# source\n",
        "# read A, B, C M2 file\n",
        "m2_file_A = '/content/wi+locness/m2/A.train.gold.bea19.m2'\n",
        "m2_file_B = '/content/wi+locness/m2/B.train.gold.bea19.m2'\n",
        "m2_file_C = '/content/wi+locness/m2/C.train.gold.bea19.m2'\n",
        "# read and concatenate all files\n",
        "m2_ABC_file = read_file(m2_file_A) + read_file(m2_file_B) + read_file(m2_file_C)\n",
        "# save to a file\n",
        "m2_file = '/content/wi+locness/m2/ABC.train.gold.bea19.m2'\n",
        "with open(m2_file, 'w') as f:\n",
        "    for line in m2_ABC_file:\n",
        "        f.write('%s\\n' %line)\n",
        "# parse M2 file\n",
        "output_file = '/content/wi+locness/m2/ABCN.train.gold.bea19.src'\n",
        "bea_train_src = m2_parser(m2_file, output_file)\n",
        "#---------------------------\n",
        "# reference\n",
        "bea_train_ref = read_file(m2_file)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qb855LhMPBPQ"
      },
      "source": [
        "### 3.4.3 Development set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LiS0Bka9PBPY",
        "colab": {}
      },
      "source": [
        "# import test set\n",
        "#---------------------------\n",
        "# source\n",
        "m2_file     = '/content/wi+locness/m2/ABCN.dev.gold.bea19.m2'\n",
        "output_file = '/content/wi+locness/m2/ABCN.dev.gold.bea19.src'\n",
        "bea_test_src = m2_parser(m2_file, output_file)\n",
        "# reference\n",
        "bea_test_ref = read_file(m2_file)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ml4lT0blLd31"
      },
      "source": [
        "### 3.4.4 Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XFlkYh9oLd32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "d29fec65-f68e-470f-b70b-7ab42fc36a83"
      },
      "source": [
        "print('original sentence:')\n",
        "print(bea_train_src[0])\n",
        "#---------------------------\n",
        "print('\\nannotation:')\n",
        "print(*bea_train_ref[0:2], sep='\\n')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original sentence:\n",
            "My town is a medium size city with eighty thousand inhabitants .\n",
            "\n",
            "annotation:\n",
            "S My town is a medium size city with eighty thousand inhabitants .\n",
            "A 5 6|||R:OTHER|||- sized|||REQUIRED|||-NONE-|||0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gNl55dGPbY0n"
      },
      "source": [
        "## 3.5 ReGRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oH09uwsFbY0s"
      },
      "source": [
        "### 3.5.1 Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "APXWwIolbY0v",
        "colab": {}
      },
      "source": [
        "# # mount drive to access file with sentences\n",
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C3PURlHbbY09"
      },
      "source": [
        "### 3.5.2 Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qae1zW-lbY0_",
        "colab": {}
      },
      "source": [
        "# # source\n",
        "# regra_src_file = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/src.txt'\n",
        "# #regra_src = read_file(regra_src_file, encoding='latin-1')\n",
        "# regra_src = read_file(regra_src_file, encoding='utf-8')\n",
        "# #---------------------------\n",
        "# # reference\n",
        "# regra_ref_file = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/ref.txt'\n",
        "# #regra_ref = read_file(regra_ref_file, encoding='latin-1')\n",
        "# regra_ref = read_file(regra_ref_file, encoding='utf-8')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qr0LNAL0bY1a"
      },
      "source": [
        "### 3.5.4 Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r7o4yiwGbY1b",
        "colab": {}
      },
      "source": [
        "# print('original sentences:')\n",
        "# print(*regra_src[1000:1003], sep='\\n')\n",
        "# #---------------------------\n",
        "# print('\\nreference sentences:')\n",
        "# print(*regra_ref[1000:1003], sep='\\n')"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHoXo3Yz_RcG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# 4. Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWWXjEvy-huM",
        "colab_type": "text"
      },
      "source": [
        "## 4.1 $M^2$ (MaxMatch) score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NxjTYMbK603",
        "colab_type": "text"
      },
      "source": [
        "### 4.1.1 Getting the $M^2$ scorer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ya-AnHp6Bil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get m2scorer\n",
        "! wget -q -nc https://www.comp.nus.edu.sg/~nlp/sw/m2scorer.tar.gz\n",
        "! tar -xzf m2scorer.tar.gz\n",
        "! rm m2scorer.tar.gz"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riHBzeryK_rE",
        "colab_type": "text"
      },
      "source": [
        "### 4.1.2 Testing the $M^2$ scorer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KgY1PtLLDIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting examples\n",
        "src = '/content/m2scorer/example/system2'\n",
        "ref = '/content/m2scorer/example/source_gold'"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh5uObNth6dO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "3c6ce0e9-ae33-4b03-ec4b-cde6a1045333"
      },
      "source": [
        "# source\n",
        "print('source sentences:')\n",
        "print(*read_file(src), sep='\\n')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source sentences:\n",
            "A cat sat on mat .\n",
            "The dog .\n",
            "Giant otters are apex predator .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdVqnVpvh8PO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "fadc341e-13dc-4062-ac2d-68f1786d4544"
      },
      "source": [
        "# reference\n",
        "print('reference sentences:')\n",
        "print(*read_file(ref), sep='\\n')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reference sentences:\n",
            "S The cat sat at mat .\n",
            "A 3 4|||Prep|||on|||REQUIRED|||-NONE-|||0\n",
            "A 4 4|||ArtOrDet|||the||a|||REQUIRED|||-NONE-|||0\n",
            "\n",
            "S The dog .\n",
            "A 1 2|||NN|||dogs|||REQUIRED|||-NONE-|||0\n",
            "A -1 -1|||noop|||-NONE-|||-NONE-|||-NONE-|||1\n",
            "\n",
            "S Giant otters is an apex predator .\n",
            "A 2 3|||SVA|||are|||REQUIRED|||-NONE-|||0\n",
            "A 3 4|||ArtOrDet|||-NONE-|||REQUIRED|||-NONE-|||0\n",
            "A 5 6|||NN|||predators|||REQUIRED|||-NONE-|||0\n",
            "A 1 2|||NN|||otter|||REQUIRED|||-NONE-|||1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP1wZZovhRHo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "09184780-f1d5-44ac-bc09-0020607021db"
      },
      "source": [
        "# score\n",
        "score = m2scorer(src, ref)\n",
        "print(score)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision   : 0.7500\n",
            "Recall      : 0.6000\n",
            "F_0.5       : 0.7143\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epfrUo1p-z2I",
        "colab_type": "text"
      },
      "source": [
        "## 4.2 GLEU score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQi0LbfvQW4g",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/keisks/jfleg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f4kque9rLHxc"
      },
      "source": [
        "### 4.2.1 Getting the GLEU scorer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxJovhR0-RlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import gleu metric\n",
        "sys.path.append('/content/jfleg/eval/')\n",
        "from gleu import GLEU\n",
        "gleu_calculator = GLEU()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JcLgb2T3LHxz"
      },
      "source": [
        "### 4.2.2 Testing the GLEU scorer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miazmHeOQFdy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "0c8b15fe-2eb1-4e70-e1af-1373283543c3"
      },
      "source": [
        "# hyp = ref\n",
        "#---------------------------\n",
        "src = 'jfleg/test/test.src'\n",
        "ref = ['jfleg/test/test.ref0']\n",
        "hyp = 'jfleg/test/test.ref0'\n",
        "print(f'GLEU = {calc_gleu(src, ref, hyp):.2f}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is one reference. NOTE: GLEU is not computing the confidence interval.\n",
            "GLEU = 100.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJLYfDIxLNfS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d4a1158a-f23f-42df-fca3-9cc9fb216808"
      },
      "source": [
        "# hyp = src\n",
        "#---------------------------\n",
        "# source file\n",
        "src = 'jfleg/test/test.src'\n",
        "# reference file\n",
        "ref = ['jfleg/test/test.ref0',\n",
        "       'jfleg/test/test.ref1',\n",
        "       'jfleg/test/test.ref2',\n",
        "       'jfleg/test/test.ref3']\n",
        "# hypothesis file\n",
        "hyp = 'jfleg/test/test.src'\n",
        "# calculate score\n",
        "print(f'GLEU = {calc_gleu(src, ref, hyp):.2f}')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GLEU = 40.47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy8VpqP6sK62",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "0f96469f-4308-4ba0-9009-61d998e74f1b"
      },
      "source": [
        "# hyp = ref\n",
        "#---------------------------\n",
        "# source file\n",
        "src = 'jfleg/test/test.src'\n",
        "#-------------\n",
        "# ref0\n",
        "hyp = 'jfleg/test/test.ref0'\n",
        "ref = ['jfleg/test/test.ref1', 'jfleg/test/test.ref2', 'jfleg/test/test.ref3']\n",
        "ref0 = calc_gleu(src, ref, hyp);\n",
        "#-------------\n",
        "# ref1\n",
        "hyp = 'jfleg/test/test.ref1'\n",
        "ref = ['jfleg/test/test.ref0', 'jfleg/test/test.ref2', 'jfleg/test/test.ref3']\n",
        "ref1 = calc_gleu(src, ref, hyp);\n",
        "#-------------\n",
        "# ref2\n",
        "hyp = 'jfleg/test/test.ref2'\n",
        "ref = ['jfleg/test/test.ref0', 'jfleg/test/test.ref1', 'jfleg/test/test.ref3']\n",
        "ref2 = calc_gleu(src, ref, hyp);\n",
        "#-------------\n",
        "# ref3\n",
        "hyp = 'jfleg/test/test.ref3'\n",
        "ref = ['jfleg/test/test.ref0', 'jfleg/test/test.ref1', 'jfleg/test/test.ref2']\n",
        "ref3 = calc_gleu(src, ref, hyp);\n",
        "#-------------\n",
        "print(f'ref0 = {ref0:.2f}')\n",
        "print(f'ref1 = {ref1:.2f}')\n",
        "print(f'ref2 = {ref2:.2f}')\n",
        "print(f'ref3 = {ref3:.2f}')\n",
        "print('#-------------')\n",
        "print(f'mean = {(ref0 + ref1 + ref2 + ref3) / 4:.2f}')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ref0 = 61.32\n",
            "ref1 = 61.48\n",
            "ref2 = 63.04\n",
            "ref3 = 63.53\n",
            "#-------------\n",
            "mean = 62.34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICl-e8YPceas",
        "colab_type": "text"
      },
      "source": [
        "reference table:  \n",
        "\n",
        "|  system   | GLEU (dev) | GLEU (test) |\n",
        "|:--------: | :--------: | :---------: |\n",
        "|  SOURCE   |   38.21    |    40.54    | \n",
        "| REFERENCE |   55.26    |    62.37    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR02HbOTNos9",
        "colab_type": "text"
      },
      "source": [
        "## 4.3 Edit distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uCbkSeSXLJ65"
      },
      "source": [
        "### 4.3.1 Getting distances algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZY1ydYP3PWSX"
      },
      "source": [
        "https://github.com/luozhouyang/python-string-similarity#damerau-levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd-DTeLkNnx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "levenshtein = get_distance_algorithm('levenshtein')\n",
        "damerau     = get_distance_algorithm('damerau')\n",
        "normalized  = get_distance_algorithm('normalized')\n",
        "weighted    = get_distance_algorithm('weighted')\n",
        "osa         = get_distance_algorithm('osa')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-K5iDjn0LJ7L"
      },
      "source": [
        "### 4.3.2 Testing Damerau-Levenshtein distance algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpwTw7wjqphX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "408b67d1-1296-4341-f517-e6c0db99bfb7"
      },
      "source": [
        "# distance = 1: character removed\n",
        "print('distance =', damerau.distance('Covid-19', 'Covid-9')) "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distance = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rWFZQppLhLm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d32664b5-ac83-426c-cc94-3e0e0682f154"
      },
      "source": [
        "# distance = 2: character removed & character inserted\n",
        "print('distance =', damerau.distance('Covid-19', 'Codiv-19')) "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distance = 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVen2fmQqdT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "4e96c7b6-b09e-4b95-b004-fb7fd0d9dce0"
      },
      "source": [
        "# distance = 1: transposition of two adjacent characters\n",
        "print('distance =', damerau.distance('Covid-19', 'Covid-91')) "
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distance = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9-v_kbtyl4rv"
      },
      "source": [
        "# 5. Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-aDopSbdaCdB"
      },
      "source": [
        "## 5.1 BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_9HuN_bXaCdC",
        "colab": {}
      },
      "source": [
        "# English\n",
        "#tokenizer = get_tokenizer('bert-base-cased')\n",
        "#tokenizer = get_tokenizer('bert-large-cased')\n",
        "#tokenizer = get_tokenizer('bert-base-cased')\n",
        "tokenizer = get_tokenizer('bert-large-cased')\n",
        "#---------------------------\n",
        "# Portuguese\n",
        "#tokenizer = get_tokenizer('neuralmind/bert-base-portuguese-cased')\n",
        "tokenizer = get_tokenizer('neuralmind/bert-large-portuguese-cased')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WStSsKuNvUdd",
        "colab_type": "text"
      },
      "source": [
        "## 5.2 T5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqgWbJvlvYrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenizer = get_tokenizer('t5-small')\n",
        "#tokenizer = get_tokenizer('t5-base')\n",
        "tokenizer = get_tokenizer('t5-large')\n",
        "#tokenizer = get_tokenizer('t5-3b')\n",
        "#tokenizer = get_tokenizer('t5-11b')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KxWrS8Qhl4r3"
      },
      "source": [
        "# 6. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fER52IqZvfJJ",
        "colab_type": "text"
      },
      "source": [
        "## 6.1 BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-7KJZa2ql4r4",
        "colab": {}
      },
      "source": [
        "# English\n",
        "#model = get_model('bert-base-cased')     # BERT base  cased   [en]  436 MB\n",
        "model = get_model('bert-large-cased')    # BERT large cased   [en] 1.34 GB\n",
        "#model = get_model('bert-base-uncased')   # BERT base  uncased [en]  440 MB\n",
        "#model = get_model('bert-large-uncased')  # BERT large uncased [en] 1.34 GB\n",
        "#---------------------------\n",
        "# Portuguese\n",
        "#model = get_model('neuralmind/bert-base-portuguese-cased')  # BERT base  cased [pt]  438 MB\n",
        "model = get_model('neuralmind/bert-large-portuguese-cased') # BERT large cased [pt] 1.34 GB "
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n5nHwFXMl4r8"
      },
      "source": [
        "## 6.2 T5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cWXQ7_spl4r9",
        "colab": {}
      },
      "source": [
        "#model = get_model('t5-small')   #  242 MB\n",
        "#model = get_model('t5-base')    #  892 MB\n",
        "model = get_model('t5-large')   # 2.95 GB\n",
        "#model = get_model('t5-3b')      # 11.4 GB\n",
        "#model = get_model('t5-11b')     # ??.? GB"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVQUMbIBPX-D",
        "colab_type": "text"
      },
      "source": [
        "# 7. Sentence Correction Suggestion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIx8oEwUQ9Mt",
        "colab_type": "text"
      },
      "source": [
        "## 7.1 T5-based function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOdWXTwlQ1y1",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uFB4dMj3buI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of output predictions\n",
        "k = 30\n",
        "# beams used in beam search\n",
        "b = 50\n",
        "# DamerauLevenshtein\n",
        "edit_distance = get_distance_algorithm('damerau')\n",
        "# threshold distance to suggest correction\n",
        "threshold = 5"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGRdT2rOVZ5P",
        "colab_type": "text"
      },
      "source": [
        "Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l7u_rUTy3Spf",
        "colab": {}
      },
      "source": [
        "def suggest_t5(sentences, tokenizer, model, distance, split=False, k=30, b=50, threshold=5, device='cpu'):\n",
        "    model.to(device)\n",
        "    sentences_suggested = []\n",
        "    for sentence in sentences:\n",
        "        #---------------------------\n",
        "        # split and add mask\n",
        "        # tokenize\n",
        "        tokenized_raw = sentence.split()\n",
        "        tokenized = tokenized_raw.copy()\n",
        "        tokenized.append('</s>')\n",
        "        # repeat tensor\n",
        "        repeated = [tokenized*1 for _ in range(len(tokenized_raw))]\n",
        "        #---------------------------\n",
        "        # mask tokens (insert '<extra_id_0>')\n",
        "        for i, seq in enumerate(repeated):\n",
        "            seq[i] = '<extra_id_0>'\n",
        "        #---------------------------\n",
        "        # joing tokens back\n",
        "        joined = []\n",
        "        for seq in repeated:\n",
        "            joined.append(' '.join(seq))\n",
        "        #---------------------------\n",
        "        # encode sentences\n",
        "        input_ids = []\n",
        "        for masked_sentence in joined:\n",
        "            input_ids.append(tokenizer.encode(masked_sentence, add_special_tokens=True, return_tensors='pt'))\n",
        "        #---------------------------\n",
        "        # top-k predictions\n",
        "        topk_pred_pt = torch.zeros((len(repeated), k))\n",
        "        for i, masked_sentence in enumerate(input_ids):\n",
        "            # model predict\n",
        "            model_output = model.generate(input_ids = masked_sentence.to(device), num_beams=b, num_return_sequences=k, max_length=3)\n",
        "            topk_pred_pt[i] = model_output[:,-1]\n",
        "        topk_pred_pt.long()\n",
        "        #---------------------------\n",
        "        # convert ids back to words\n",
        "        topk_pred_tokens = []   # list of lists\n",
        "        for masked_sentence in topk_pred_pt:\n",
        "            pred_list = []\n",
        "            for predictions in masked_sentence:\n",
        "                pred_list.append(tokenizer.decode([predictions.tolist()]))\n",
        "            topk_pred_tokens.append(pred_list)\n",
        "        topk_pred_tokens\n",
        "        #---------------------------\n",
        "        # compare predictions and calculate edit distance\n",
        "        suggestion = []\n",
        "        for i, masked_token in enumerate(tokenized_raw):\n",
        "            # check if masked token is in predictions\n",
        "            if masked_token in topk_pred_tokens[i]:\n",
        "                # if it is, no correction is suggested\n",
        "                suggestion.append(masked_token)\n",
        "            #---------------------------\n",
        "            else:    \n",
        "                # using distance?\n",
        "                if (distance != None):\n",
        "                    # if masked token not in predictions, calculate distance\n",
        "                    dist = torch.zeros(k)\n",
        "                    for j, prediction in enumerate(topk_pred_tokens[i]):\n",
        "                        dist[j] = edit_distance.distance(masked_token, prediction)\n",
        "                    # check if minimum distance is under a limiar\n",
        "                    if torch.min(dist).item() <= threshold:\n",
        "                        # if it is, make suggestions\n",
        "                        # argmin returns the last index --> workaround: flip the tensor\n",
        "                        min_index = len(dist) - torch.argmin(dist.flip(0)).item() - 1\n",
        "                        suggestion.append(topk_pred_tokens[i][min_index])\n",
        "                    #---------------------------\n",
        "                    else:\n",
        "                        # if it is not, make no correction suggestion\n",
        "                        suggestion.append(masked_token)\n",
        "                #---------------------------\n",
        "                # greedy suggestion\n",
        "                else: \n",
        "                    suggestion.append(topk_pred_tokens[i][0])\n",
        "        #---------------------------\n",
        "        sentences_suggested.append(' '.join(suggestion))\n",
        "    return sentences_suggested"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sMswTaV3fAS",
        "colab_type": "text"
      },
      "source": [
        "## 7.2 Step-by-Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTR7ApzS5jTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.to(device);"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU5ZJIBO61wC",
        "colab_type": "text"
      },
      "source": [
        "### 7.2.0 Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqTCK0605_nn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of output predictions\n",
        "k = 5\n",
        "# beams used in beam search\n",
        "b = 10\n",
        "# DamerauLevenshtein\n",
        "edit_distance = get_distance_algorithm('damerau')\n",
        "# threshold distance to suggest correction\n",
        "threshold = 2"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APNALzgx40IU",
        "colab_type": "text"
      },
      "source": [
        "### 7.2.1 Get sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4d3gjItrpPP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "047dc3b7-4fef-4d21-a474-ecd8f6ebc4cd"
      },
      "source": [
        "# get sentence\n",
        "sentence = jfleg_train_src[721]\n",
        "sentence"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'People also do not do nothing . '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyCXN_Xb4ujP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "a5c7d944-b685-40cb-85a0-7861d30ed5aa"
      },
      "source": [
        "# get references\n",
        "print(jfleg_train_ref0[721])\n",
        "print(jfleg_train_ref1[721])\n",
        "print(jfleg_train_ref2[721])\n",
        "print(jfleg_train_ref3[721])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "People also do not do anything . \n",
            "People also do not do nothing . \n",
            "People also do something . \n",
            "People also do not do nothing . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KactA01e42d3",
        "colab_type": "text"
      },
      "source": [
        "### 7.2.2 Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZIgi77Jrvw7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3e7f072c-e222-47d5-88a1-ed9fdb8f77f9"
      },
      "source": [
        "# tokenize\n",
        "tokenized_raw = sentence.split()\n",
        "tokenized = tokenized_raw.copy()\n",
        "tokenized.append('</s>')\n",
        "tokenized"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['People', 'also', 'do', 'not', 'do', 'nothing', '.', '</s>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyV8JMvXGqht",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "outputId": "eb5ddcae-5748-480d-d416-144b9b7b987d"
      },
      "source": [
        "# repeat tensor\n",
        "repeated = [tokenized*1 for _ in range(len(tokenized_raw))]\n",
        "repeated"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['People', 'also', 'do', 'not', 'do', 'nothing', '.', '</s>'],\n",
              " ['People', 'also', 'do', 'not', 'do', 'nothing', '.', '</s>'],\n",
              " ['People', 'also', 'do', 'not', 'do', 'nothing', '.', '</s>'],\n",
              " ['People', 'also', 'do', 'not', 'do', 'nothing', '.', '</s>'],\n",
              " ['People', 'also', 'do', 'not', 'do', 'nothing', '.', '</s>'],\n",
              " ['People', 'also', 'do', 'not', 'do', 'nothing', '.', '</s>'],\n",
              " ['People', 'also', 'do', 'not', 'do', 'nothing', '.', '</s>']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTt0HOhO5Kgn",
        "colab_type": "text"
      },
      "source": [
        "### 7.2.3 Mask tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6wu3S5ctbdA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "outputId": "c87c7849-39fb-4ff8-e805-9d400760f5e0"
      },
      "source": [
        "# insert '<extra_id_0>'\n",
        "for i, seq in enumerate(repeated):\n",
        "    seq[i] = '<extra_id_0>'\n",
        "repeated"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<extra_id_0>', 'also', 'do', 'not', 'do', 'nothing', '.', '</s>'],\n",
              " ['People', '<extra_id_0>', 'do', 'not', 'do', 'nothing', '.', '</s>'],\n",
              " ['People', 'also', '<extra_id_0>', 'not', 'do', 'nothing', '.', '</s>'],\n",
              " ['People', 'also', 'do', '<extra_id_0>', 'do', 'nothing', '.', '</s>'],\n",
              " ['People', 'also', 'do', 'not', '<extra_id_0>', 'nothing', '.', '</s>'],\n",
              " ['People', 'also', 'do', 'not', 'do', '<extra_id_0>', '.', '</s>'],\n",
              " ['People', 'also', 'do', 'not', 'do', 'nothing', '<extra_id_0>', '</s>']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuntudKUCKkZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "outputId": "a8260e25-4f13-4492-8d0b-be727616a75a"
      },
      "source": [
        "# joing tokens back\n",
        "joined = []\n",
        "for seq in repeated:\n",
        "    joined.append(' '.join(seq))\n",
        "joined"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<extra_id_0> also do not do nothing . </s>',\n",
              " 'People <extra_id_0> do not do nothing . </s>',\n",
              " 'People also <extra_id_0> not do nothing . </s>',\n",
              " 'People also do <extra_id_0> do nothing . </s>',\n",
              " 'People also do not <extra_id_0> nothing . </s>',\n",
              " 'People also do not do <extra_id_0> . </s>',\n",
              " 'People also do not do nothing <extra_id_0> </s>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZRXAieoChhP",
        "colab_type": "text"
      },
      "source": [
        "### 7.2.4 Encoding sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdNXbSi_CiLZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "outputId": "54d6fd08-7fe5-4411-a4a9-015ad967e07d"
      },
      "source": [
        "input_ids = []\n",
        "for masked_sentence in joined:\n",
        "    input_ids.append(tokenizer.encode(masked_sentence, add_special_tokens=True, return_tensors='pt'))\n",
        "input_ids\n"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[32099,    92,   103,    59,   103,  1327,     3,     5,     1]]),\n",
              " tensor([[ 2449, 32099,   103,    59,   103,  1327,     3,     5,     1]]),\n",
              " tensor([[ 2449,    92, 32099,    59,   103,  1327,     3,     5,     1]]),\n",
              " tensor([[ 2449,    92,   103, 32099,   103,  1327,     3,     5,     1]]),\n",
              " tensor([[ 2449,    92,   103,    59, 32099,  1327,     3,     5,     1]]),\n",
              " tensor([[ 2449,    92,   103,    59,   103, 32099,     3,     5,     1]]),\n",
              " tensor([[ 2449,    92,   103,    59,   103,  1327, 32099,     1]])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os5vqcPI4fpm",
        "colab_type": "text"
      },
      "source": [
        "### 7.2.5 Top-k predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-zIEMFAC29I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "outputId": "127fb7a6-6568-4133-dae6-02623af74118"
      },
      "source": [
        "topk_pred_pt = torch.zeros((len(repeated), k))\n",
        "for i, masked_sentence in enumerate(input_ids):\n",
        "    # model predict\n",
        "    model_output = model.generate(input_ids = masked_sentence.to(device), num_beams=b, num_return_sequences=k, max_length=3)\n",
        "    topk_pred_pt[i] = model_output[:,-1]\n",
        "topk_pred_pt.long()\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  11,   68,    3,   25,   62],\n",
              "        [ 113,    3,   33,  103,    6],\n",
              "        [ 103,   54,  225,   56,  410],\n",
              "        [  59,  424, 1327,    3,  378],\n",
              "        [ 103,  214,   43,  241,  114],\n",
              "        [   3,   34,    8,   48,  959],\n",
              "        [   5,   55,  535,   58, 1280]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bxu0LI0fFPiG",
        "colab_type": "text"
      },
      "source": [
        "### 7.2.6 Convert IDs back to words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfEHSodoE9FM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "outputId": "8b35433d-e0ca-435f-e94d-f49dc858e80d"
      },
      "source": [
        "# convert ids back to words\n",
        "topk_pred_tokens = []   # list of lists\n",
        "for masked_sentence in topk_pred_pt:\n",
        "    pred_list = []\n",
        "    for predictions in masked_sentence:\n",
        "        pred_list.append(tokenizer.decode([predictions.tolist()]))\n",
        "    topk_pred_tokens.append(pred_list)\n",
        "topk_pred_tokens"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['and', 'but', '', 'you', 'we'],\n",
              " ['who', '', 'are', 'do', ','],\n",
              " ['do', 'can', 'should', 'will', 'did'],\n",
              " ['not', 'something', 'nothing', '', 'things'],\n",
              " ['do', 'know', 'have', 'want', 'like'],\n",
              " ['', 'it', 'the', 'this', 'anything'],\n",
              " ['.', '!', '.\"', '?', '\".']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5wHYttZ8Vgk",
        "colab_type": "text"
      },
      "source": [
        "### 7.2.7 Compare predictions and calculate distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g9o0cDarhpO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "9a29afed-a5fc-489b-b38c-dac9d9efafb9"
      },
      "source": [
        "# compare predictions and calculate edit distance\n",
        "suggestion = []\n",
        "for i, masked_token in enumerate(tokenized_raw):\n",
        "    # check if masked token is in predictions\n",
        "    if masked_token in topk_pred_tokens[i]:\n",
        "        # if it is, no correction is suggested\n",
        "        suggestion.append(masked_token)\n",
        "    #---------------------------\n",
        "    else:    \n",
        "        # using distance?\n",
        "        if (edit_distance != None):\n",
        "            # if masked token not in predictions, calculate distance\n",
        "            dist = torch.zeros(k)\n",
        "            for j, prediction in enumerate(topk_pred_tokens[i]):\n",
        "                dist[j] = edit_distance.distance(masked_token, prediction)\n",
        "            # check if minimum distance is under a limiar\n",
        "            if torch.min(dist).item() <= threshold:\n",
        "                # if it is, make suggestions\n",
        "                # argmin returns the last index --> workaround: flip the tensor\n",
        "                min_index = len(dist) - torch.argmin(dist.flip(0)).item() - 1\n",
        "                suggestion.append(topk_pred_tokens[i][min_index])\n",
        "            #---------------------------\n",
        "            else:\n",
        "                # if it is not, make no correction suggestion\n",
        "                suggestion.append(masked_token)\n",
        "        #---------------------------\n",
        "        # greedy suggestion\n",
        "        else: \n",
        "            suggestion.append(topk_pred_tokens[i][0])\n",
        "#---------------------------\n",
        "' '.join(suggestion)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'People also do not do anything .'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PtuXpG9wRimA"
      },
      "source": [
        "# End of the notebook"
      ]
    }
  ]
}