{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NSGC - Neural Spell & Grammar Checker (en/pt).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aibQMqn3rEp8",
        "colab_type": "text"
      },
      "source": [
        "# PF06 - NSGC: Neural Spell & Grammar Checker (en/pt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "399F2GKforfi",
        "colab_type": "text"
      },
      "source": [
        "Author: **Rafael Ito**  \n",
        "e-mail: ito.rafael@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV7swfVuBvgm",
        "colab_type": "text"
      },
      "source": [
        "# 0. Dataset and Description\n",
        "\n",
        "**Name:**  CoNLL-2014, JFLEG, BEA  \n",
        "**Description:** in this notebook we will use BERT and T5 to predict words in a sentence to perform a spell and grammar checker for Portuguese and English languages. For English, we will use the BERT and T5 models from transformers library (huggingface) and evaluate the performance in CoNLL-2014 and JFLEG datasets. For Portuguese, we will use the transformers/neuralmind BERT version and a custom dataset for evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnLtgkmhr0y0",
        "colab_type": "text"
      },
      "source": [
        "# 1. Libraries and packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rmn8887vHg2C",
        "colab_type": "text"
      },
      "source": [
        "## 1.1 Check device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXc7eeN9HjNh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "641af068-027e-4444-a585-d84b4d652cce"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    device_model = torch.cuda.get_device_name(0)\n",
        "print('GPU model:', device_model)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU model: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF94hPU1HWt9",
        "colab_type": "text"
      },
      "source": [
        "## 1.2 Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44YWHZaqwDo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# install Python libs\n",
        "!pip install -q     \\\n",
        "    numpy           \\\n",
        "    torch           \\\n",
        "    transformers    \n",
        "#----------------------------\n",
        "# install PyEnchant\n",
        "! apt-get -qq update\n",
        "! apt-get -qq install libenchant-dev\n",
        "! pip install -q pyenchant\n",
        "#----------------------------\n",
        "# string similarity and distance\n",
        "! pip install -q strsimpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-lNBFD0HcPb",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYPw4vBWq_nd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "d8cbb47a-4651-4d6f-e46b-e107cb0c34c9"
      },
      "source": [
        "#-------------------------------------------------\n",
        "# general\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "import os\n",
        "import pdb\n",
        "import codecs\n",
        "import subprocess\n",
        "from multiprocessing import cpu_count\n",
        "#-------------------------------------------------\n",
        "# NLP\n",
        "from transformers import T5Tokenizer, BertTokenizer, BertForMaskedLM, T5ForConditionalGeneration\n",
        "import enchant\n",
        "import nltk\n",
        "nltk.download('words')\n",
        "from nltk.corpus import words\n",
        "#-------------------------------------------------\n",
        "# Edit distance algorithms\n",
        "from strsimpy.levenshtein import Levenshtein\n",
        "from strsimpy.normalized_levenshtein import NormalizedLevenshtein\n",
        "from strsimpy.weighted_levenshtein import WeightedLevenshtein\n",
        "from strsimpy.weighted_levenshtein import CharacterSubstitutionInterface\n",
        "from strsimpy.damerau import Damerau\n",
        "from strsimpy.optimal_string_alignment import OptimalStringAlignment\n",
        "#-------------------------------------------------\n",
        "# random seed generator\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "#-------------------------------------------------\n",
        "# Suppress some of the logging\n",
        "import logging\n",
        "logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARNING)\n",
        "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.WARNING)\n",
        "#-------------------------------------------------\n",
        "# Suppress warning messages\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "#-------------------------------------------------\n",
        "# package version\n",
        "print('Torch version:', torch.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "Torch version: 1.5.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XxgMICcyNGTR"
      },
      "source": [
        "## 1.4 Device info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iq0yV3NmNGTj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        },
        "outputId": "a55fbbf6-e005-4e5c-eaf2-76985a1eff40"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    device_model = torch.cuda.get_device_name(0)\n",
        "    device_memory = torch.cuda.get_device_properties(device).total_memory / 1e9\n",
        "#----------------------------\n",
        "print('Device:', device)\n",
        "print('GPU model:', device_model)\n",
        "print('GPU memory: {0:.2f} GB'.format(device_memory))\n",
        "print('#-------------------')\n",
        "print('CPU cores:', cpu_count())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device: cuda\n",
            "GPU model: Tesla P100-PCIE-16GB\n",
            "GPU memory: 17.07 GB\n",
            "#-------------------\n",
            "CPU cores: 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8cTjVNEoJkJ",
        "colab_type": "text"
      },
      "source": [
        "# 2. Custom functions and classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrRWKsWkAkdU",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Function to read file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbNoiarV9Iuz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function that reads a file and return its text\n",
        "#------------------------------------------------------\n",
        "parameters:\n",
        "    - path: path of the file to be read\n",
        "    - encoding: encoding to be used\n",
        "returns:\n",
        "    file content as list of strings\n",
        "'''\n",
        "def read_file(path, encoding='utf-8'):\n",
        "    with codecs.open(path, encoding=encoding) as f:\n",
        "        return f.read().splitlines()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpyM_L_ly9Yc",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Function to write in file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLCa0dLi0tqy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function that writes list of strings in a file\n",
        "#------------------------------------------------------\n",
        "parameters:\n",
        "    - sentences: list of strings to be written in file\n",
        "    - path: path of the file where strings will be written\n",
        "returns:\n",
        "    path: same as input\n",
        "'''\n",
        "def write_file(sentences, path, encoding='utf-8'):\n",
        "    with codecs.open(path, 'w', encoding=encoding) as f:\n",
        "        for sentence in sentences:\n",
        "            f.write(sentence + '\\n')\n",
        "    f.close()\n",
        "    return path"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l5phhds3aCYP"
      },
      "source": [
        "## 2.3 Function to get tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qJOiUWH4aCYQ",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function that returns the tokenizer associated to a string\n",
        "#------------------------------------------------------\n",
        "parameters:\n",
        "    tokenizer:\n",
        "      BERT options:\n",
        "        - 'bert-base-cased'\n",
        "        - 'bert-large-cased'\n",
        "        - 'bert-base-uncased'\n",
        "        - 'bert-large-uncased'\n",
        "      T5 options:\n",
        "        - 't5-small'\n",
        "        - 't5-base'\n",
        "        - 't5-large'\n",
        "        - 't5-3b'\n",
        "        - 't5-11b'\n",
        "      otherwise raise an error\n",
        "returns:\n",
        "    Hugging Face's tokenizer\n",
        "'''\n",
        "def get_tokenizer(tokenizer):\n",
        "    # BERT\n",
        "    if ((tokenizer == 'bert-base-cased') or \n",
        "        (tokenizer == 'bert-large-cased') or\n",
        "        (tokenizer == 'bert-base-uncased') or \n",
        "        (tokenizer == 'bert-large-uncased') or\n",
        "        (tokenizer == 'neuralmind/bert-large-portuguese-cased') or \n",
        "        (tokenizer == 'neuralmind/bert-base-portuguese-cased')):\n",
        "        return BertTokenizer.from_pretrained(tokenizer)\n",
        "    #------------------------------------------------------\n",
        "    # T5\n",
        "    elif ((tokenizer == 't5-small') or \n",
        "          (tokenizer == 't5-base') or \n",
        "          (tokenizer == 't5-large') or \n",
        "          (tokenizer == 't5-3b') or \n",
        "          (tokenizer == 't5-11b')):\n",
        "        return T5Tokenizer.from_pretrained(tokenizer)\n",
        "    #------------------------------------------------------\n",
        "    else:\n",
        "        raise ValueError(f'Unsupported tokenizer: {tokenizer}')\n"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dti6F_ksRO47",
        "colab_type": "text"
      },
      "source": [
        "## 2.4 Function to get model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCah31PHQMHP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function that returns the the network model associated to a string\n",
        "#------------------------------------------------------\n",
        "parameters:\n",
        "    model_name:\n",
        "      BERT models:\n",
        "        - 'bert-base-cased'                         # BERT base  cased   [en] (110 M params)\n",
        "        - 'bert-large-cased'                        # BERT large cased   [en] (340 M params)\n",
        "        - 'bert-base-uncased'                       # BERT base  uncased [en] (110 M params)\n",
        "        - 'bert-large-uncased'                      # BERT large uncased [en] (340 M params)\n",
        "        - 'neuralmind/bert-base-portuguese-cased'   # BERT base  cased   [pt] (110 M params)\n",
        "        - 'neuralmind/bert-large-portuguese-cased'  # BERT large cased   [pt] (340 M params)\n",
        "      T5 models:\n",
        "        - 't5-small' (60 M params)\n",
        "        - 't5-base'  (220 M params)\n",
        "        - 't5-large' (770 M params)\n",
        "        - 't5-3B'    (2.8 B params)\n",
        "        - 't5-11B'   (11 B params)\n",
        "      otherwise raise an error\n",
        "returns:\n",
        "    Hugging Face's model\n",
        "'''\n",
        "def get_model(model_name):\n",
        "    # BERT\n",
        "    if ((model_name == 'bert-base-cased') or                        # BERT base  cased   [en]\n",
        "        (model_name == 'bert-large-cased') or                       # BERT large cased   [en]\n",
        "        (model_name == 'bert-base-uncased') or                      # BERT base  uncased [en]\n",
        "        (model_name == 'bert-large-uncased') or                     # BERT large uncased [en]\n",
        "        (model_name == 'neuralmind/bert-base-portuguese-cased') or  # BERT base  cased   [pt]\n",
        "        (model_name == 'neuralmind/bert-large-portuguese-cased')):  # BERT large cased   [pt]\n",
        "        return BertForMaskedLM.from_pretrained(model_name)\n",
        "    #------------------------------------------------------\n",
        "    # T5\n",
        "    elif ((model_name == 't5-small') or     # T5 small [en]   242 MB\n",
        "          (model_name == 't5-base')  or     # T5 base  [en]   892 MB\n",
        "          (model_name == 't5-large') or     # T5 large [en]  2.95 GB\n",
        "          (model_name == 't5-3B')    or     # T5 3B    [en]  11.4 GB\n",
        "          (model_name == 't5-11B')):        # T5 11B   [en]  ??.? GB\n",
        "        return T5ForConditionalGeneration.from_pretrained(model_name, use_bfloat16=True)\n",
        "    #------------------------------------------------------\n",
        "    else:\n",
        "        raise ValueError(f'Unsupported model: {model_name}')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c93U-7ppa9Z",
        "colab_type": "text"
      },
      "source": [
        "## 2.5 Function to edit distance algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TI2HyauqlhLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function that returns the algorithm to calculate the edit distance\n",
        "#------------------------------------------------------\n",
        "parameters:                   +--------------------------+---------+\n",
        "    algorithm:                |        algorithm         | metric? |\n",
        "                              +--------------------------+---------+\n",
        "        - 'levenshtein'       | Levenshtein              |   yes   |\n",
        "        - 'normalized'        | Normalized Levenshtein   |   no    |\n",
        "        - 'weighted'          | Weighted Levenshtein     |   no    |\n",
        "        - 'damerau'           | Damerau-Levenshtein      |   yes   |\n",
        "        - 'osa'               | Optimal String Alignment |   no    |\n",
        "    otherwise raise an error  +--------------------------+---------+\n",
        "returns:\n",
        "    edit distance algorithm\n",
        "'''\n",
        "def get_distance_algorithm(algorithm):\n",
        "    if (algorithm == 'levenshtein'):\n",
        "        return Levenshtein()\n",
        "    elif (algorithm == 'normalized'):\n",
        "        return NormalizedLevenshtein()\n",
        "    elif (algorithm == 'weighted'):\n",
        "        return \n",
        "    elif (algorithm == 'damerau'):\n",
        "        return Damerau()\n",
        "    elif (algorithm == 'osa'):\n",
        "        return OptimalStringAlignment()\n",
        "    else:\n",
        "        raise ValueError(f'Unsupported algorithm: {algorithm}')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6YO8zkSPzWe",
        "colab_type": "text"
      },
      "source": [
        "## 2.6 Function to calculate GLEU score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raq4l8OAdXfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function that receives text files and calculate GLEU score\n",
        "#------------------------------------------------------\n",
        "parameters:\n",
        "    - src: source file\n",
        "    - ref: reference file(s)\n",
        "    - hyp: hypothesis file\n",
        "    - n: n-gram order\n",
        "    - num_iter: number of GLEU iterations\n",
        "    - sent: sentence level scores\n",
        "returns:\n",
        "    GLEU score (float)\n",
        "'''\n",
        "def calc_gleu(src, ref, hyp, n=4, num_iter=500, sent=False):\n",
        "    gleu_calculator.load_sources(src)\n",
        "    gleu_calculator.load_references(ref)\n",
        "    if len(ref) == 1:\n",
        "        print(\"There is one reference. NOTE: GLEU is not computing the confidence interval.\")\n",
        "        gleu = [g for g in gleu_calculator.run_iterations(\n",
        "            num_iterations=num_iter,\n",
        "            source=src,\n",
        "            hypothesis=hyp,\n",
        "            per_sent=sent)][0][0] \n",
        "    else:\n",
        "        gleu = [g for g in gleu_calculator.run_iterations(\n",
        "            num_iterations=num_iter,\n",
        "            source=src,\n",
        "            hypothesis=hyp,\n",
        "            per_sent=sent)][0][0]\n",
        "    #print(gleu)\n",
        "    return float(gleu)*100"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K85K64M_7Vo2",
        "colab_type": "text"
      },
      "source": [
        "## 2.7 Function to calculate MaxMatch score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ-tjeb-5JW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function that runs Python 2 script to calculate M^2 score\n",
        "#------------------------------------------------------\n",
        "parameters:\n",
        "    - src_file_path: source file path\n",
        "    - ref_file_path: reference file path\n",
        "returns:\n",
        "    MaxMatch score (precision, recall, F_{0.5}) as string\n",
        "'''\n",
        "def m2scorer(src_file_path, ref_file_path):\n",
        "    process = subprocess.Popen(['/content/m2scorer/scripts/m2scorer.py', src_file_path, ref_file_path], stdout=subprocess.PIPE)\n",
        "    output, error = process.communicate()\n",
        "    output = output.decode(\"utf-8\")\n",
        "    return output"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c97AaIugm5CT",
        "colab_type": "text"
      },
      "source": [
        "## 2.8 Function parse M2 file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McGSJNNafjDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "function that receives M2 format file and returns original sentences\n",
        "#------------------------------------------------------\n",
        "parameters:\n",
        "    - m2_file: reference file in M2 format\n",
        "    - output_file: file where the output will be written\n",
        "returns:\n",
        "    list of strings with original sentences\n",
        "'''\n",
        "def m2_parser(m2_file, output_file):\n",
        "    # create output file\n",
        "    !touch /content/conll14st-test-data/noalt/official-2014.1.cor\n",
        "    # delete annotations, blank lines and 'S ' at the beginning of sentences\n",
        "    !sed -e '/^A/d' -e '/^$/d' -e 's/^S //g' $m2_file > $output_file\n",
        "    # read output file and return it as list of string\n",
        "    conll_2014_test_src = read_file(output_file)\n",
        "    return conll_2014_test_src"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FNiPmd7_j_h",
        "colab_type": "text"
      },
      "source": [
        "# 3. Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UWXy8iYIbPGQ"
      },
      "source": [
        "## 3.1 CoNLL-2013"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HPQMtQKtbPGW"
      },
      "source": [
        "### 3.1.1 Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l9ZWF4GMbPGb",
        "colab": {}
      },
      "source": [
        "# test set\n",
        "! wget -q -nc https://www.comp.nus.edu.sg/~nlp/conll13st/release2.3.1.tar.gz\n",
        "! tar -xzf release2.3.1.tar.gz\n",
        "! rm release2.3.1.tar.gz"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-rN8qr49bPG3"
      },
      "source": [
        "### 3.1.2 Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vLxelb0BbPG4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "a9c9e720-bfab-461e-cc86-8ec420379a56"
      },
      "source": [
        "# import test set\n",
        "#---------------------------\n",
        "# source\n",
        "m2_file     = '/content/release2.3.1/revised/data/official-preprocessed.m2'\n",
        "output_file = '/content/release2.3.1/revised/data/official-preprocessed.src'\n",
        "conll_2013_test_src = m2_parser(m2_file, output_file)\n",
        "# reference\n",
        "conll_2013_test_ref = read_file(m2_file)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "touch: cannot touch '/content/conll14st-test-data/noalt/official-2014.1.cor': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hpRqjctlbPHA"
      },
      "source": [
        "### 3.1.3 Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y8nkpoxDbPHB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "44f8a593-87ff-411f-a861-2938983bac09"
      },
      "source": [
        "print('original sentence:')\n",
        "print(conll_2013_test_src[0])\n",
        "#---------------------------\n",
        "print('\\nannotation:')\n",
        "print(*conll_2013_test_ref[0:4], sep='\\n')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original sentence:\n",
            "In modern digital world , electronic products are widely used in daily lives such as Smart phones , computers and etc .\n",
            "\n",
            "annotation:\n",
            "S In modern digital world , electronic products are widely used in daily lives such as Smart phones , computers and etc .\n",
            "A 1 1|||ArtOrDet|||the|||REQUIRED|||-NONE-|||0\n",
            "A 12 13|||Nn|||life|||REQUIRED|||-NONE-|||0\n",
            "A 15 16|||Mec|||smart|||REQUIRED|||-NONE-|||0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjVSbN-jllk3",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 CoNLL-2014"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmtSxziZOKnO",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.1 Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70DO7E2GOMsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## training set\n",
        "#from google.colab import drive\n",
        "#drive.mount('/gdrive')\n",
        "#---------------------------\n",
        "# test set\n",
        "! wget -q -nc https://www.comp.nus.edu.sg/~nlp/conll14st/conll14st-test-data.tar.gz\n",
        "! tar -xzf conll14st-test-data.tar.gz\n",
        "! rm conll14st-test-data.tar.gz"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5gY1fUrBQTM",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.2 Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4Zqmc5B_nRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # import training set\n",
        "# #---------------------------\n",
        "# source\n",
        "# m2_file     = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/CoNLL-2014/release3.3/data/conll14st-preprocessed.m2'\n",
        "# output_file = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/CoNLL-2014/release3.3/data/conll14st-preprocessed.src'\n",
        "# conll_2014_test_src = m2_parser(m2_file, output_file)\n",
        "# # reference\n",
        "# conll_2014_train_ref = read_file(m2_file)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48DvLgUL-YWQ",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.3 Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PklOfNoe-aBM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import test set\n",
        "#---------------------------\n",
        "# source\n",
        "m2_file     = '/content/conll14st-test-data/noalt/official-2014.1.m2'\n",
        "output_file = '/content/conll14st-test-data/noalt/official-2014.1.src'\n",
        "conll_2014_test_src = m2_parser(m2_file, output_file)\n",
        "# reference\n",
        "conll_2014_test_ref = read_file(m2_file)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND1owATCBSVv",
        "colab_type": "text"
      },
      "source": [
        "### 3.2.4 Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LDPER-iJwC3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "a069acb1-ba5a-48e9-d0f5-c4ee287305d7"
      },
      "source": [
        "print('original sentence:')\n",
        "print(conll_2014_test_src[3])\n",
        "#---------------------------\n",
        "print('\\nannotation:')\n",
        "print(*conll_2014_test_ref[7:9], sep='\\n')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original sentence:\n",
            "People get certain disease because of genetic changes .\n",
            "\n",
            "annotation:\n",
            "S People get certain disease because of genetic changes .\n",
            "A 3 4|||Nn|||diseases|||REQUIRED|||-NONE-|||0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEVfMbnflpjD",
        "colab_type": "text"
      },
      "source": [
        "## 3.3 JFLEG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3c37lULNtt2",
        "colab_type": "text"
      },
      "source": [
        "### 3.3.1 Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhChJtUzNvYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clone GitHub repo\n",
        "! git clone --quiet https://github.com/keisks/jfleg.git 2> /dev/null"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_j57PTqKiPX",
        "colab_type": "text"
      },
      "source": [
        "### 3.3.2 Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HM1uGY0xAawo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import training set\n",
        "#---------------------------\n",
        "# source\n",
        "jfleg_train_src = read_file('jfleg/dev/dev.src')\n",
        "# references\n",
        "jfleg_train_ref0 = read_file('jfleg/dev/dev.ref0')\n",
        "jfleg_train_ref1 = read_file('jfleg/dev/dev.ref1')\n",
        "jfleg_train_ref2 = read_file('jfleg/dev/dev.ref2')\n",
        "jfleg_train_ref3 = read_file('jfleg/dev/dev.ref3')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZL0W3y5CNcex"
      },
      "source": [
        "### 3.3.3 Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3sqG-fWFNce3",
        "colab": {}
      },
      "source": [
        "# import test set\n",
        "#---------------------------\n",
        "# source\n",
        "jfleg_test_src = read_file('jfleg/test/test.src')\n",
        "# references\n",
        "jfleg_test_ref0 = read_file('jfleg/test/test.ref0')\n",
        "jfleg_test_ref1 = read_file('jfleg/test/test.ref1')\n",
        "jfleg_test_ref2 = read_file('jfleg/test/test.ref2')\n",
        "jfleg_test_ref3 = read_file('jfleg/test/test.ref3')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUNVKEuFKoXH",
        "colab_type": "text"
      },
      "source": [
        "### 3.3.4 Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYVBq0Lk99J2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "9f59daac-27f0-4f2f-dc21-8dc84a6ea7a5"
      },
      "source": [
        "# print source and references example\n",
        "print('source sentence:')\n",
        "print(jfleg_test_src[0])\n",
        "#---------------------------\n",
        "print('\\nreferences sentences:')\n",
        "print(jfleg_test_ref0[0])\n",
        "print(jfleg_test_ref1[0])\n",
        "print(jfleg_test_ref2[0])\n",
        "print(jfleg_test_ref3[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source sentence:\n",
            "New and new technology has been introduced to the society .\n",
            "\n",
            "references sentences:\n",
            "New technology has been introduced to society .\n",
            "New technology has been introduced into the society .\n",
            "Newer and newer technology has been introduced into society .\n",
            "Newer and newer technology has been introduced to the society .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAA_8tJh_nBj",
        "colab_type": "text"
      },
      "source": [
        "## 3.4 BEA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbbURbluKq-b",
        "colab_type": "text"
      },
      "source": [
        "### 3.4.1 Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UotyF7HwwRgE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download test data\n",
        "! wget -q -nc https://www.cl.cam.ac.uk/research/nl/bea2019st/data/wi+locness_v2.1.bea19.tar.gz\n",
        "! tar -xzf wi+locness_v2.1.bea19.tar.gz\n",
        "! rm wi+locness_v2.1.bea19.tar.gz"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CLbTX4UWLd3c"
      },
      "source": [
        "### 3.4.2 Training set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lShaa-b4Ld3m",
        "colab": {}
      },
      "source": [
        "# import test set\n",
        "#---------------------------\n",
        "# source\n",
        "# read A, B, C M2 file\n",
        "m2_file_A = '/content/wi+locness/m2/A.train.gold.bea19.m2'\n",
        "m2_file_B = '/content/wi+locness/m2/B.train.gold.bea19.m2'\n",
        "m2_file_C = '/content/wi+locness/m2/C.train.gold.bea19.m2'\n",
        "# read and concatenate all files\n",
        "m2_ABC_file = read_file(m2_file_A) + read_file(m2_file_B) + read_file(m2_file_C)\n",
        "# save to a file\n",
        "m2_file = '/content/wi+locness/m2/ABC.train.gold.bea19.m2'\n",
        "with open(m2_file, 'w') as f:\n",
        "    for line in m2_ABC_file:\n",
        "        f.write('%s\\n' %line)\n",
        "# parse M2 file\n",
        "output_file = '/content/wi+locness/m2/ABCN.train.gold.bea19.src'\n",
        "bea_train_src = m2_parser(m2_file, output_file)\n",
        "#---------------------------\n",
        "# reference\n",
        "bea_train_ref = read_file(m2_file)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qb855LhMPBPQ"
      },
      "source": [
        "### 3.4.3 Development set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LiS0Bka9PBPY",
        "colab": {}
      },
      "source": [
        "# import test set\n",
        "#---------------------------\n",
        "# source\n",
        "m2_file     = '/content/wi+locness/m2/ABCN.dev.gold.bea19.m2'\n",
        "output_file = '/content/wi+locness/m2/ABCN.dev.gold.bea19.src'\n",
        "bea_test_src = m2_parser(m2_file, output_file)\n",
        "# reference\n",
        "bea_test_ref = read_file(m2_file)"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ml4lT0blLd31"
      },
      "source": [
        "### 3.4.4 Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XFlkYh9oLd32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "d88de4b3-f497-42b1-efc6-ca54e9a1bdd0"
      },
      "source": [
        "print('original sentence:')\n",
        "print(bea_train_src[0])\n",
        "#---------------------------\n",
        "print('\\nannotation:')\n",
        "print(*bea_train_ref[0:2], sep='\\n')"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original sentence:\n",
            "It 's difficult answer at the question \" what are you going to do in the future ? \" if the only one who has to know it is in two minds .\n",
            "\n",
            "annotation:\n",
            "S My town is a medium size city with eighty thousand inhabitants .\n",
            "A 5 6|||R:OTHER|||- sized|||REQUIRED|||-NONE-|||0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gNl55dGPbY0n"
      },
      "source": [
        "## 3.5 ReGRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oH09uwsFbY0s"
      },
      "source": [
        "### 3.5.1 Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "APXWwIolbY0v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "37028633-7586-44a1-bdf1-f7737b4a9f22"
      },
      "source": [
        "# mount drive to access file with sentences\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C3PURlHbbY09"
      },
      "source": [
        "### 3.5.2 Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qae1zW-lbY0_",
        "colab": {}
      },
      "source": [
        "# source\n",
        "regra_src_file = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/src.txt'\n",
        "#regra_src = read_file(regra_src_file, encoding='latin-1')\n",
        "regra_src = read_file(regra_src_file, encoding='utf-8')\n",
        "#---------------------------\n",
        "# reference\n",
        "regra_ref_file = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/ref.txt'\n",
        "#regra_ref = read_file(regra_ref_file, encoding='latin-1')\n",
        "regra_ref = read_file(regra_ref_file, encoding='utf-8')"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qr0LNAL0bY1a"
      },
      "source": [
        "### 3.5.4 Sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r7o4yiwGbY1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "379cae63-16a4-49d4-fa16-d844f44f4a39"
      },
      "source": [
        "print('original sentences:')\n",
        "print(*regra_src[1000:1003], sep='\\n')\n",
        "#---------------------------\n",
        "print('\\nreference sentences:')\n",
        "print(*regra_ref[1000:1003], sep='\\n')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original sentences:\n",
            "Uma delegação de padeiros vem prestar seu apoio as mulheres dos grevistas.\n",
            "Uma era ítala-brasileira.\n",
            "Uma frota de navios norte-americanos se dirigiste ao Mar Mediterrâneo.\n",
            "\n",
            "reference sentences:\n",
            "Uma delegação de padeiros vem prestar seu apoio às mulheres dos grevistas.\n",
            "Uma era ítalo-brasileira.\n",
            "Uma frota de navios norte-americanos se dirige ao Mar Mediterrâneo.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHoXo3Yz_RcG",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# 4. Evaluation Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWWXjEvy-huM",
        "colab_type": "text"
      },
      "source": [
        "## 4.1 $M^2$ (MaxMatch) score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NxjTYMbK603",
        "colab_type": "text"
      },
      "source": [
        "### 4.1.1 Getting the $M^2$ scorer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ya-AnHp6Bil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get m2scorer\n",
        "! wget -q -nc https://www.comp.nus.edu.sg/~nlp/sw/m2scorer.tar.gz\n",
        "! tar -xzf m2scorer.tar.gz\n",
        "! rm m2scorer.tar.gz"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riHBzeryK_rE",
        "colab_type": "text"
      },
      "source": [
        "### 4.1.2 Testing the $M^2$ scorer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KgY1PtLLDIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting examples\n",
        "src = '/content/m2scorer/example/system2'\n",
        "ref = '/content/m2scorer/example/source_gold'"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh5uObNth6dO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "3808deee-a732-43c9-dd90-37faa55eab77"
      },
      "source": [
        "# source\n",
        "print('source sentences:')\n",
        "print(*read_file(src), sep='\\n')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source sentences:\n",
            "A cat sat on mat .\n",
            "The dog .\n",
            "Giant otters are apex predator .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdVqnVpvh8PO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "61e86a5e-e3af-4ffd-aef9-377e83d83dab"
      },
      "source": [
        "# reference\n",
        "print('reference sentences:')\n",
        "print(*read_file(ref), sep='\\n')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reference sentences:\n",
            "S The cat sat at mat .\n",
            "A 3 4|||Prep|||on|||REQUIRED|||-NONE-|||0\n",
            "A 4 4|||ArtOrDet|||the||a|||REQUIRED|||-NONE-|||0\n",
            "\n",
            "S The dog .\n",
            "A 1 2|||NN|||dogs|||REQUIRED|||-NONE-|||0\n",
            "A -1 -1|||noop|||-NONE-|||-NONE-|||-NONE-|||1\n",
            "\n",
            "S Giant otters is an apex predator .\n",
            "A 2 3|||SVA|||are|||REQUIRED|||-NONE-|||0\n",
            "A 3 4|||ArtOrDet|||-NONE-|||REQUIRED|||-NONE-|||0\n",
            "A 5 6|||NN|||predators|||REQUIRED|||-NONE-|||0\n",
            "A 1 2|||NN|||otter|||REQUIRED|||-NONE-|||1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP1wZZovhRHo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "3b8cfc19-4249-409b-db52-20a204151bd2"
      },
      "source": [
        "# score\n",
        "score = m2scorer(src, ref)\n",
        "print(score)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision   : 0.7500\n",
            "Recall      : 0.6000\n",
            "F_0.5       : 0.7143\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epfrUo1p-z2I",
        "colab_type": "text"
      },
      "source": [
        "## 4.2 GLEU score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQi0LbfvQW4g",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/keisks/jfleg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f4kque9rLHxc"
      },
      "source": [
        "### 4.2.1 Getting the GLEU scorer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxJovhR0-RlQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import gleu metric\n",
        "sys.path.append('/content/jfleg/eval/')\n",
        "from gleu import GLEU\n",
        "gleu_calculator = GLEU()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JcLgb2T3LHxz"
      },
      "source": [
        "### 4.2.2 Testing the GLEU scorer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miazmHeOQFdy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "e7cb7e6b-fed0-46c1-9d76-ee14c9d0a8ba"
      },
      "source": [
        "# hyp = ref\n",
        "#---------------------------\n",
        "src = 'jfleg/test/test.src'\n",
        "ref = ['jfleg/test/test.ref0']\n",
        "hyp = 'jfleg/test/test.ref0'\n",
        "print(f'GLEU = {calc_gleu(src, ref, hyp):.2f}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is one reference. NOTE: GLEU is not computing the confidence interval.\n",
            "GLEU = 100.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJLYfDIxLNfS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "7ff47893-79cc-4725-9c82-82f18d10a4ac"
      },
      "source": [
        "# hyp = src\n",
        "#---------------------------\n",
        "# source file\n",
        "src = 'jfleg/test/test.src'\n",
        "# reference file\n",
        "ref = ['jfleg/test/test.ref0',\n",
        "       'jfleg/test/test.ref1',\n",
        "       'jfleg/test/test.ref2',\n",
        "       'jfleg/test/test.ref3']\n",
        "# hypothesis file\n",
        "hyp = 'jfleg/test/test.src'\n",
        "# calculate score\n",
        "print(f'GLEU = {calc_gleu(src, ref, hyp):.2f}')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GLEU = 40.47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy8VpqP6sK62",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "51336ae0-2655-407f-dcc7-485bb50d8a9e"
      },
      "source": [
        "# hyp = ref\n",
        "#---------------------------\n",
        "# source file\n",
        "src = 'jfleg/test/test.src'\n",
        "#-------------\n",
        "# ref0\n",
        "hyp = 'jfleg/test/test.ref0'\n",
        "ref = ['jfleg/test/test.ref1', 'jfleg/test/test.ref2', 'jfleg/test/test.ref3']\n",
        "ref0 = calc_gleu(src, ref, hyp);\n",
        "#-------------\n",
        "# ref1\n",
        "hyp = 'jfleg/test/test.ref1'\n",
        "ref = ['jfleg/test/test.ref0', 'jfleg/test/test.ref2', 'jfleg/test/test.ref3']\n",
        "ref1 = calc_gleu(src, ref, hyp);\n",
        "#-------------\n",
        "# ref2\n",
        "hyp = 'jfleg/test/test.ref2'\n",
        "ref = ['jfleg/test/test.ref0', 'jfleg/test/test.ref1', 'jfleg/test/test.ref3']\n",
        "ref2 = calc_gleu(src, ref, hyp);\n",
        "#-------------\n",
        "# ref3\n",
        "hyp = 'jfleg/test/test.ref3'\n",
        "ref = ['jfleg/test/test.ref0', 'jfleg/test/test.ref1', 'jfleg/test/test.ref2']\n",
        "ref3 = calc_gleu(src, ref, hyp);\n",
        "#-------------\n",
        "print(f'ref0 = {ref0:.2f}')\n",
        "print(f'ref1 = {ref1:.2f}')\n",
        "print(f'ref2 = {ref2:.2f}')\n",
        "print(f'ref3 = {ref3:.2f}')\n",
        "print('#-------------')\n",
        "print(f'mean = {(ref0 + ref1 + ref2 + ref3) / 4:.2f}')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ref0 = 61.32\n",
            "ref1 = 61.48\n",
            "ref2 = 63.04\n",
            "ref3 = 63.53\n",
            "#-------------\n",
            "mean = 62.34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICl-e8YPceas",
        "colab_type": "text"
      },
      "source": [
        "reference table:  \n",
        "\n",
        "|  system   | GLEU (dev) | GLEU (test) |\n",
        "|:--------: | :--------: | :---------: |\n",
        "|  SOURCE   |   38.21    |    40.54    | \n",
        "| REFERENCE |   55.26    |    62.37    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR02HbOTNos9",
        "colab_type": "text"
      },
      "source": [
        "## 4.3 Edit distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uCbkSeSXLJ65"
      },
      "source": [
        "### 4.3.1 Getting distances algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZY1ydYP3PWSX"
      },
      "source": [
        "https://github.com/luozhouyang/python-string-similarity#damerau-levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd-DTeLkNnx0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "levenshtein = get_distance_algorithm('levenshtein')\n",
        "damerau     = get_distance_algorithm('damerau')\n",
        "normalized  = get_distance_algorithm('normalized')\n",
        "weighted    = get_distance_algorithm('weighted')\n",
        "osa         = get_distance_algorithm('osa')"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-K5iDjn0LJ7L"
      },
      "source": [
        "### 4.3.2 Testing Damerau-Levenshtein distance algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpwTw7wjqphX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "bc514762-be2c-4b4a-c35d-f0d2bb055fa1"
      },
      "source": [
        "# distance = 1: character removed\n",
        "print('distance =', damerau.distance('Covid-19', 'Covid-9')) "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distance = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rWFZQppLhLm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "93e17f67-8276-437b-f84f-1b7abf804300"
      },
      "source": [
        "# distance = 2: character removed & character inserted\n",
        "print('distance =', damerau.distance('Covid-19', 'Codiv-19')) "
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distance = 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVen2fmQqdT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6e6862fe-4d86-41f8-a7aa-a363e29f3a7d"
      },
      "source": [
        "# distance = 1: transposition of two adjacent characters\n",
        "print('distance =', damerau.distance('Covid-19', 'Covid-91')) "
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "distance = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GH-wNnp3wOMM",
        "colab_type": "text"
      },
      "source": [
        "# 5. Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-aDopSbdaCdB"
      },
      "source": [
        "## 5.1 BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_9HuN_bXaCdC",
        "colab": {}
      },
      "source": [
        "# English\n",
        "#tokenizer = get_tokenizer('bert-base-cased')\n",
        "#tokenizer = get_tokenizer('bert-large-cased')\n",
        "#tokenizer = get_tokenizer('bert-base-cased')\n",
        "tokenizer = get_tokenizer('bert-large-cased')\n",
        "#---------------------------\n",
        "# Portuguese\n",
        "#tokenizer = get_tokenizer('neuralmind/bert-base-portuguese-cased')\n",
        "tokenizer = get_tokenizer('neuralmind/bert-large-portuguese-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WStSsKuNvUdd",
        "colab_type": "text"
      },
      "source": [
        "## 5.2 T5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqgWbJvlvYrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenizer = get_tokenizer('t5-small')\n",
        "#tokenizer = get_tokenizer('t5-base')\n",
        "tokenizer = get_tokenizer('t5-large')\n",
        "#tokenizer = get_tokenizer('t5-3b')\n",
        "#tokenizer = get_tokenizer('t5-11b')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sBJ0QdFt3hG",
        "colab_type": "text"
      },
      "source": [
        "# 6. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fER52IqZvfJJ",
        "colab_type": "text"
      },
      "source": [
        "## 6.1 BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSYDbLcAyT7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# English\n",
        "#model = get_model('bert-base-cased')     # BERT base  cased   [en]  436 MB\n",
        "model = get_model('bert-large-cased')    # BERT large cased   [en] 1.34 GB\n",
        "#model = get_model('bert-base-uncased')   # BERT base  uncased [en]  440 MB\n",
        "#model = get_model('bert-large-uncased')  # BERT large uncased [en] 1.34 GB\n",
        "#---------------------------\n",
        "# Portuguese\n",
        "#model = get_model('neuralmind/bert-base-portuguese-cased')  # BERT base  cased [pt]  438 MB\n",
        "model = get_model('neuralmind/bert-large-portuguese-cased') # BERT large cased [pt] 1.34 GB "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhNvJPogubis",
        "colab_type": "text"
      },
      "source": [
        "## 6.2 T5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq6lMoPDxyfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model = get_model('t5-small')   #  242 MB\n",
        "#model = get_model('t5-base')    #  892 MB\n",
        "model = get_model('t5-large')   # 2.95 GB\n",
        "#model = get_model('t5-3b')      # 11.4 GB\n",
        "#model = get_model('t5-11b')     # ??.? GB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVQUMbIBPX-D",
        "colab_type": "text"
      },
      "source": [
        "# 7. Sentence Correction Suggestion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0n7nTxQHzOoF"
      },
      "source": [
        "## 7.1 BERT-based function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aRT2DqZgVtUS"
      },
      "source": [
        "For a step-to-step explained algorithm, please check:  \n",
        "https://colab.research.google.com/drive/1xXo-jMTFctcBOeVpClb9J8ddqHDnM6Jz?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E16xX1UsNczU",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LgwpF75dzOod",
        "colab": {}
      },
      "source": [
        "# topk model output predictions used to compare\n",
        "k = 10\n",
        "# Damerau–Levenshtein\n",
        "edit_distance = get_distance_algorithm('damerau')\n",
        "# threshold distance to suggest correction\n",
        "threshold = 5"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhobf1m7V5xR",
        "colab_type": "text"
      },
      "source": [
        "Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g9o0cDarhpO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def suggest_bert(sentences, tokenizer, model, distance, split=False, k=20, threshold=5, device='cpu', T5=False):\n",
        "    model.to(device)\n",
        "    sentences_suggested = []\n",
        "    for sentence in sentences:\n",
        "        #---------------------------\n",
        "        # tokenize\n",
        "        if split:\n",
        "            tokenized = sentence.split()                                # dummy tokenizer\n",
        "        else: \n",
        "            tokenized = tokenizer.tokenize(sentence)                    # tokenize\n",
        "        tokenized_ids = tokenizer.encode(tokenized)                     # '[CLS]' + get word ids + '[SEP]'\n",
        "        single_input_ids = torch.LongTensor(tokenized_ids).to(device)   # convert list to tensor\n",
        "        input_ids = single_input_ids.repeat(len(single_input_ids)-2, 1) # repeat tensor\n",
        "        #---------------------------\n",
        "        # mask tokens\n",
        "        for i in range(len(input_ids)):\n",
        "            input_ids[i][i+1] = tokenizer.mask_token_id\n",
        "        #---------------------------\n",
        "        # predict the top-k tokens for the masked ones\n",
        "        topk_pred_pt = torch.zeros((len(tokenized), k))\n",
        "        for i, masked_sentence in enumerate(input_ids): \n",
        "            model_output = model(input_ids = masked_sentence.unsqueeze(dim=0))\n",
        "            logits = model_output[0]\n",
        "            _, predicted_ids = torch.topk(logits, k, sorted=True)\n",
        "            topk_pred_pt[i] = predicted_ids.squeeze()[i+1]\n",
        "        #---------------------------\n",
        "        # convert ids back to words\n",
        "        topk_pred_tokens = []   # list of lists\n",
        "        for masked_sentence in topk_pred_pt:\n",
        "            pred_list = []\n",
        "            for predictions in masked_sentence:\n",
        "                pred_list.append(tokenizer.decode([predictions.tolist()]))\n",
        "            topk_pred_tokens.append(pred_list)\n",
        "        #---------------------------\n",
        "        # compare predictions and calculate edit distance\n",
        "        suggestion = []\n",
        "        for i, masked_token in enumerate(tokenized):\n",
        "            # check if masked token is in predictions\n",
        "            if masked_token in topk_pred_tokens[i]:\n",
        "                # if it is, no correction is suggested\n",
        "                suggestion.append(masked_token)\n",
        "            #---------------------------\n",
        "            else:    \n",
        "                # using distance?\n",
        "                if (distance != None):\n",
        "                    # if masked token not in predictions, calculate distance\n",
        "                    dist = torch.zeros(k)\n",
        "                    for j, prediction in enumerate(topk_pred_tokens[i]):\n",
        "                        dist[j] = edit_distance.distance(masked_token, prediction)\n",
        "                    # check if minimum distance is under a limiar\n",
        "                    if torch.min(dist).item() <= threshold:\n",
        "                        # if it is, make suggestions\n",
        "                        # argmin returns the last index --> workaround: flip the tensor\n",
        "                        min_index = len(dist) - torch.argmin(dist.flip(0)).item() - 1\n",
        "                        suggestion.append(topk_pred_tokens[i][min_index])\n",
        "                    #---------------------------\n",
        "                    else:\n",
        "                        # if it is not, make no correction suggestion\n",
        "                        suggestion.append(masked_token)\n",
        "                #---------------------------\n",
        "                # greedy suggestion\n",
        "                else: \n",
        "                    suggestion.append(topk_pred_tokens[i][0])\n",
        "        #---------------------------\n",
        "        sentences_suggested.append(' '.join(suggestion))\n",
        "    return sentences_suggested"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhsc6GbEBGGq",
        "colab_type": "text"
      },
      "source": [
        "## 7.2 T5-based function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNhfrS2kNe0J",
        "colab_type": "text"
      },
      "source": [
        "For a step-to-step explained algorithm, please check:  \n",
        "https://colab.research.google.com/drive/1CsIdhgM5zo_0_W4f1lSndUk8tKyMfx_g?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW-QzzHONYFp",
        "colab_type": "text"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbVuJquekTob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# number of output predictions\n",
        "k = 30\n",
        "# beams used in beam search\n",
        "b = 50\n",
        "# Damerau–Levenshtein\n",
        "edit_distance = get_distance_algorithm('damerau')\n",
        "# threshold distance to suggest correction\n",
        "threshold = 5"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jtPnyVDQOpT",
        "colab_type": "text"
      },
      "source": [
        "Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5h5WSjaXzUAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def suggest_t5(sentences, tokenizer, model, distance, split=False, k=30, b=50, threshold=5, device='cpu'):\n",
        "    model.to(device)\n",
        "    sentences_suggested = []\n",
        "    for sentence in sentences:\n",
        "        #---------------------------\n",
        "        # split and add mask\n",
        "        # tokenize\n",
        "        tokenized_raw = sentence.split()\n",
        "        tokenized = tokenized_raw.copy()\n",
        "        tokenized.append('</s>')\n",
        "        # repeat tensor\n",
        "        repeated = [tokenized*1 for _ in range(len(tokenized_raw))]\n",
        "        #---------------------------\n",
        "        # mask tokens (insert '<extra_id_0>')\n",
        "        for i, seq in enumerate(repeated):\n",
        "            seq[i] = '<extra_id_0>'\n",
        "        #---------------------------\n",
        "        # joing tokens back\n",
        "        joined = []\n",
        "        for seq in repeated:\n",
        "            joined.append(' '.join(seq))\n",
        "        #---------------------------\n",
        "        # encode sentences\n",
        "        input_ids = []\n",
        "        for masked_sentence in joined:\n",
        "            input_ids.append(tokenizer.encode(masked_sentence, add_special_tokens=True, return_tensors='pt'))\n",
        "        #---------------------------\n",
        "        # top-k predictions\n",
        "        topk_pred_pt = torch.zeros((len(repeated), k))\n",
        "        for i, masked_sentence in enumerate(input_ids):\n",
        "            # model predict\n",
        "            model_output = model.generate(input_ids = masked_sentence.to(device), num_beams=b, num_return_sequences=k, max_length=3)\n",
        "            topk_pred_pt[i] = model_output[:,-1]\n",
        "        topk_pred_pt.long()\n",
        "        #---------------------------\n",
        "        # convert ids back to words\n",
        "        topk_pred_tokens = []   # list of lists\n",
        "        for masked_sentence in topk_pred_pt:\n",
        "            pred_list = []\n",
        "            for predictions in masked_sentence:\n",
        "                pred_list.append(tokenizer.decode([predictions.tolist()]))\n",
        "            topk_pred_tokens.append(pred_list)\n",
        "        topk_pred_tokens\n",
        "        #---------------------------\n",
        "        # compare predictions and calculate edit distance\n",
        "        suggestion = []\n",
        "        for i, masked_token in enumerate(tokenized_raw):\n",
        "            # check if masked token is in predictions\n",
        "            if masked_token in topk_pred_tokens[i]:\n",
        "                # if it is, no correction is suggested\n",
        "                suggestion.append(masked_token)\n",
        "            #---------------------------\n",
        "            else:    \n",
        "                # using distance?\n",
        "                if (distance != None):\n",
        "                    # if masked token not in predictions, calculate distance\n",
        "                    dist = torch.zeros(k)\n",
        "                    for j, prediction in enumerate(topk_pred_tokens[i]):\n",
        "                        dist[j] = edit_distance.distance(masked_token, prediction)\n",
        "                    # check if minimum distance is under a limiar\n",
        "                    if torch.min(dist).item() <= threshold:\n",
        "                        # if it is, make suggestions\n",
        "                        # argmin returns the last index --> workaround: flip the tensor\n",
        "                        min_index = len(dist) - torch.argmin(dist.flip(0)).item() - 1\n",
        "                        suggestion.append(topk_pred_tokens[i][min_index])\n",
        "                    #---------------------------\n",
        "                    else:\n",
        "                        # if it is not, make no correction suggestion\n",
        "                        suggestion.append(masked_token)\n",
        "                #---------------------------\n",
        "                # greedy suggestion\n",
        "                else: \n",
        "                    suggestion.append(topk_pred_tokens[i][0])\n",
        "        #---------------------------\n",
        "        sentences_suggested.append(' '.join(suggestion))\n",
        "    return sentences_suggested"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6aK9evCbmB2",
        "colab_type": "text"
      },
      "source": [
        "# 8. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZyoyAGGQzEU",
        "colab_type": "text"
      },
      "source": [
        "## 8.1 English"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XsS0UpQxtXP",
        "colab_type": "text"
      },
      "source": [
        "### 8.1.1 Using BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-fFh0hnxzAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting tokenizer and model\n",
        "tokenizer = get_tokenizer('bert-large-cased')\n",
        "model = get_model('bert-large-cased')\n",
        "model.to(device);\n",
        "#---------------------------\n",
        "# hyperparameters\n",
        "edit_distance = get_distance_algorithm('damerau')"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZwBoGvmFZNU7"
      },
      "source": [
        "#### CoNLL-2013"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zKR6ny4A6qXG"
      },
      "source": [
        "##### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FL8Swizt6qXO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "1fa0ba9e-9258-474c-8f98-9649db785d9b"
      },
      "source": [
        "# hyp = src\n",
        "#---------------------------\n",
        "# file paths\n",
        "src = '/content/release2.3.1/revised/data/official-preprocessed.src'\n",
        "#ref = ...\n",
        "m2 = '/content/release2.3.1/revised/data/official-preprocessed.m2'\n",
        "hyp = '/content/release2.3.1/revised/data/official-preprocessed.src'\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 1.0000\n",
            "Recall      : 0.0000\n",
            "F_0.5       : 0.0000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yWfOji0gZNU9"
      },
      "source": [
        "##### Test #1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qnbC5hElZNVA",
        "colab": {}
      },
      "source": [
        "threshold = 2\n",
        "k = 10 "
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6lJjvtblZNVM",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = conll_2013_test_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "986Ku67wZNVW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "c0a5a504-9bba-464e-da9d-0f5bc13e7827"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/release2.3.1/revised/data/official-preprocessed.src'\n",
        "#ref = ...\n",
        "m2 = '/content/release2.3.1/revised/data/official-preprocessed.m2'\n",
        "hyp = write_file(suggestion, '/content/release2.3.1/revised/data/official-preprocessed-En-BERT_test1_th=2,k=10.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_BERT_CoNLL-2013_test1_(th=2,k=10).txt'"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 0.2312\n",
            "Recall      : 0.0650\n",
            "F_0.5       : 0.1530\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Gc2O-A5q_dK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "73f3b81b-8d18-4e05-9c4e-fe3bdfa277b4"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[0:5], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[0:5], sep='\\n')"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "In modern digital world , electronic products are widely used in daily lives such as Smart phones , computers and etc .\n",
            "In work places , electronic devices such as computers are also inevitable to use to increase the productivity of the corporation .\n",
            "The convenience and high efficiency of using electronic products is being noticed by people worldwide .\n",
            "Some people started to think if electronic products can be further operated to more advanced utilization and replace human beings for better performances .\n",
            "Surveillance technology such as RFID ( radio-frequency identification ) is one type of examples that has currently been implemented .\n",
            "\n",
            "correction:\n",
            "In modern digital world , electronic products are widely used in daily lives such as smart phones , computers and etc .\n",
            "In work places , electronic devices such as computers are also inevitable to use to increase the productivity of the corporation .\n",
            "The convenience and high efficiency of using electronic products is being noticed by people worldwide .\n",
            "Some people started to think if electronic products can be further operated to more advanced utilization and replace human beings for better performance .\n",
            "Surveillance technology such as ID ( radio-frequency identification ) is one type of examples that has currently been implemented .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3CSy_GVwz6TB"
      },
      "source": [
        "##### Test #2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pzxUb6kNz6TH",
        "colab": {}
      },
      "source": [
        "threshold = 3\n",
        "k = 20 "
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DmUpFIFUz6TU",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = conll_2013_test_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EMyzeTBWz6Tk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "be1659e9-313e-4a77-f6ec-e57294124892"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/release2.3.1/revised/data/official-preprocessed.src'\n",
        "#ref = ...\n",
        "m2 = '/content/release2.3.1/revised/data/official-preprocessed.m2'\n",
        "hyp = write_file(suggestion, '/content/release2.3.1/revised/data/official-preprocessed-En-BERT_test2_th=3,k=20.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_BERT_CoNLL-2013_test2_(th=3,k=20).txt'"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 0.1604\n",
            "Recall      : 0.0799\n",
            "F_0.5       : 0.1335\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RnGL6MB-z6Tx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "7ff08287-9db1-4f69-eb72-1e5a74c32528"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[0:5], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[0:5], sep='\\n')"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "In modern digital world , electronic products are widely used in daily lives such as Smart phones , computers and etc .\n",
            "In work places , electronic devices such as computers are also inevitable to use to increase the productivity of the corporation .\n",
            "The convenience and high efficiency of using electronic products is being noticed by people worldwide .\n",
            "Some people started to think if electronic products can be further operated to more advanced utilization and replace human beings for better performances .\n",
            "Surveillance technology such as RFID ( radio-frequency identification ) is one type of examples that has currently been implemented .\n",
            "\n",
            "correction:\n",
            "In modern digital world , electronic products are widely used in daily lives such as smart phones , computers and TV .\n",
            "In some places , electronic devices such as computers are also inevitable to use to increase the productivity of the corporation .\n",
            "The convenience and high efficiency of using electronic products is being noticed by people worldwide .\n",
            "Some people started to think if electronic products can be further upgraded to more advanced utilization and replace human beings for theater performance .\n",
            "Surveillance technology such as ID ( radio-frequency identification ) is one type of examples that has currently been implemented .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22OQphN7zy2w",
        "colab_type": "text"
      },
      "source": [
        "#### CoNLL-2014"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CcsQJl_46uPb"
      },
      "source": [
        "##### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i80ftTAr6uPk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "20b87a0c-5a7e-41d5-f64f-2b4305a714c0"
      },
      "source": [
        "# hyp = src\n",
        "#---------------------------\n",
        "# file paths\n",
        "src = '/content/conll14st-test-data/noalt/official-2014.1.src'\n",
        "#ref = ...\n",
        "m2 = '/content/conll14st-test-data/noalt/official-2014.1.m2'\n",
        "hyp = '/content/conll14st-test-data/noalt/official-2014.1.src'\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 1.0000\n",
            "Recall      : 0.0000\n",
            "F_0.5       : 0.0000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vr3e8KAHZlke"
      },
      "source": [
        "##### Test #1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_FijQ59BZlkk",
        "colab": {}
      },
      "source": [
        "threshold = 2\n",
        "k = 10 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xaU12BXhZlkx",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = conll_2014_test_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VM3bwe_pZllA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "0fa3dd11-aea1-4d5c-9617-324a322cee22"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/conll14st-test-data/noalt/official-2014.1.src'\n",
        "#ref = ...\n",
        "m2 = '/content/conll14st-test-data/noalt/official-2014.1.m2'\n",
        "hyp = write_file(suggestion, '/content/conll14st-test-data/noalt/official-2014.1-En_BERT_test1_th=2,k=10.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_BERT_CoNLL-2014_test1_(th=2,k=10).txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 0.2635\n",
            "Recall      : 0.0838\n",
            "F_0.5       : 0.1844\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwBJ5kjndO61",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "d1079893-07fb-46ba-eefe-11347d62adec"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[0:5], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[0:5], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "Keeping the Secret of Genetic Testing\n",
            "What is genetic risk ?\n",
            "Genetic risk refers more to your chance of inheriting a disorder or disease .\n",
            "People get certain disease because of genetic changes .\n",
            "How much a genetic change tells us about your chance of developing a disorder is not always clear .\n",
            "\n",
            "correction:\n",
            "Keeping the secret of Genetic Testing\n",
            "What is genetic risk ?\n",
            "Genetic risk refers more to your chance of inheriting a disorder or disease .\n",
            "People get certain diseases because of genetic changes .\n",
            "How much a genetic change tells us about your chance of developing a disorder is not always clear .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vDbPyXRNZllI"
      },
      "source": [
        "##### Test #2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h3nM5xqbdgw_",
        "colab": {}
      },
      "source": [
        "threshold = 4\n",
        "k = 10 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iiQRBGT1dgxO",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = conll_2014_test_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nDNSzSlrdgxa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "dbe18ff4-ae4c-452c-f0f3-02f9a1171ea1"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/conll14st-test-data/noalt/official-2014.1.src'\n",
        "#ref = ...\n",
        "m2 = '/content/conll14st-test-data/noalt/official-2014.1.m2'\n",
        "hyp = write_file(suggestion, '/content/conll14st-test-data/noalt/official-2014.1-En_BERT_test2_th=4,k=10.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_BERT_CoNLL-2014_test2_(th=4,k=10).txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 0.1599\n",
            "Recall      : 0.1215\n",
            "F_0.5       : 0.1504\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p43m9w7Rdgxl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "aad86943-c2ca-403f-eb6b-30815cea5aae"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[1000:1005], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[1000:1005], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "Undeniable , it becomes more addicting when we spend more time busy socialising and interacting virtually .\n",
            "We spend majority of our time on sites like Facebook , Twitter and it affects our daily work productivity and performance .\n",
            "In corporate world , employers often block social media network to prevent employees to spend their office time on their personal leisure than concentrating on their work .\n",
            "Using text-messaging language as an informal way of communicating on social media network also brings in a bad impact for us in a long term .\n",
            "The more time we spend on these sites , the lesser time we spend on face-to-face interacting with one another .\n",
            "\n",
            "correction:\n",
            "Undeniable , it becomes more addicting when we spend more time in socialising and interacting virtually .\n",
            "We spend majority of our time on sites like Facebook , Twitter and it affects our daily work productivity and performance .\n",
            "In corporate work , employers often block social media network to prevent employees to spend their free time on their personal leisure than concentrate on their work .\n",
            "Using text-messaging language as an informal way of communicating on social media networks also ends in a big image for it in a long term .\n",
            "The more time we spend on these games , the less time we spend on face-to-face interactions with one another .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lU4SDSFygYz4"
      },
      "source": [
        "##### Test #3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LyrJMpU3gYz-",
        "colab": {}
      },
      "source": [
        "threshold = 4\n",
        "k = 20 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mFiSanJIgY0K",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = conll_2014_test_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TG3A3j37gY0U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "60a8dacf-40cd-4105-acee-db879cad7b0c"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/conll14st-test-data/noalt/official-2014.1.src'\n",
        "#ref = ...\n",
        "m2 = '/content/conll14st-test-data/noalt/official-2014.1.m2'\n",
        "hyp = write_file(suggestion, '/content/conll14st-test-data/noalt/official-2014.1-En_BERT_test3_th=4,k=20.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_BERT_CoNLL-2014_test2_(th=4,k=20).txt'\n",
        "hyp = write_file(suggestion, '/content/conll14st-test-data/noalt/official-2014.1-En_BERT_test3_th=4,k=20.cor')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 0.1618\n",
            "Recall      : 0.1109\n",
            "F_0.5       : 0.1482\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i5gcaz7GgY0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "70332d37-fca5-4b17-f84a-39eed141422f"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[1000:1005], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[1000:1005], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "Undeniable , it becomes more addicting when we spend more time busy socialising and interacting virtually .\n",
            "We spend majority of our time on sites like Facebook , Twitter and it affects our daily work productivity and performance .\n",
            "In corporate world , employers often block social media network to prevent employees to spend their office time on their personal leisure than concentrating on their work .\n",
            "Using text-messaging language as an informal way of communicating on social media network also brings in a bad impact for us in a long term .\n",
            "The more time we spend on these sites , the lesser time we spend on face-to-face interacting with one another .\n",
            "\n",
            "correction:\n",
            "Undeniable , it becomes more addicting when we spend more time in socialising and interacting literally .\n",
            "We spend majority of our time on sites like Facebook , Twitter and it affects our daily work productivity and performance .\n",
            "In corporate work , employers often allow social media networks to prevent employees to spend their free time on their personal leisure than concentrating on their work .\n",
            "Using text-messaging language as an informal way of communicating on social media network also brings in a big image for us in a long term .\n",
            "The more time we spend on these items , the lesser time we spend on face-to-face interacting with one another .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ag5srZ_Wgiwc"
      },
      "source": [
        "##### Test #4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BE2hh5Xdgiwf",
        "colab": {}
      },
      "source": [
        "threshold = 6\n",
        "k = 10 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dxpsQ_4fgiwv",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = conll_2014_test_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9sZVjKVFgiw4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "0c04fd4d-726a-4774-c998-9d03d141f7c5"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/conll14st-test-data/noalt/official-2014.1.src'\n",
        "#ref = ...\n",
        "m2 = '/content/conll14st-test-data/noalt/official-2014.1.m2'\n",
        "hyp = write_file(suggestion, '/content/conll14st-test-data/noalt/official-2014.1-En_BERT_test2_th=6,k=10.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_BERT_CoNLL-2014_test2_(th=6,k=10).txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 0.1376\n",
            "Recall      : 0.1357\n",
            "F_0.5       : 0.1372\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UCg_tPGvgiw_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "52fbee93-14e3-48ef-d5fb-cedb2417f886"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[1000:1005], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[1000:1005], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "Undeniable , it becomes more addicting when we spend more time busy socialising and interacting virtually .\n",
            "We spend majority of our time on sites like Facebook , Twitter and it affects our daily work productivity and performance .\n",
            "In corporate world , employers often block social media network to prevent employees to spend their office time on their personal leisure than concentrating on their work .\n",
            "Using text-messaging language as an informal way of communicating on social media network also brings in a bad impact for us in a long term .\n",
            "The more time we spend on these sites , the lesser time we spend on face-to-face interacting with one another .\n",
            "\n",
            "correction:\n",
            "Undeniable , it becomes more effective when we spend more time in living and interacting virtually .\n",
            "We spend majority of our time on sites like Facebook , Twitter and it affects our daily work productivity and performance .\n",
            "In corporate work , employers often use social media sites to prevent employees to spend their free time on their personal leisure than concentrate on their work .\n",
            "Using text-messaging language as an informal way of communicating on social media networks also ends in a big image for it in a long term .\n",
            "The more time we spend on these games , the less time we spend on face-to-face interactions with one another .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TSu9-XNmlvyr"
      },
      "source": [
        "#### JFLEG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3a7FiSD6QHq",
        "colab_type": "text"
      },
      "source": [
        "##### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxMoYi806Q2q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1673795c-ec98-416a-d954-42c0abcce265"
      },
      "source": [
        "# hyp = src\n",
        "#---------------------------\n",
        "# source file\n",
        "src = 'jfleg/test/test.src'\n",
        "# reference file\n",
        "ref = ['jfleg/test/test.ref0',\n",
        "       'jfleg/test/test.ref1',\n",
        "       'jfleg/test/test.ref2',\n",
        "       'jfleg/test/test.ref3']\n",
        "# hypothesis file\n",
        "hyp = 'jfleg/test/test.src'\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "GLEU_score = calc_gleu(src, ref, hyp)\n",
        "print(f'GLUE score = {GLEU_score:.2f}')"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GLUE score = 40.47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xs_VyZoDlvyy"
      },
      "source": [
        "##### Test #1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k1HNNBHUlvy0",
        "colab": {}
      },
      "source": [
        "threshold = 2\n",
        "k = 10 "
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kXnqA0julvzC",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = jfleg_test_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FATC2bR4lvzK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e705aa8a-fd82-4207-bc26-84ae2b899eae"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/jfleg/test/test.src'\n",
        "ref = ['/content/jfleg/test/test.ref0',\n",
        "       '/content/jfleg/test/test.ref1',\n",
        "       '/content/jfleg/test/test.ref2',\n",
        "       '/content/jfleg/test/test.ref3']\n",
        "#m2 = '...'\n",
        "hyp = write_file(suggestion, '/content/jfleg/test/test-En_BERT_test1_th=2,k=10.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "GLEU_score = calc_gleu(src, ref, hyp)\n",
        "print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "#M2_score = m2scorer(hyp, m2)\n",
        "#print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_BERT_JFLEG_test1_(th=2,k=10).txt'"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GLUE score = 44.56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KLNvYIWVlvzR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "d1079893-07fb-46ba-eefe-11347d62adec"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[0:5], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[0:5], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "Keeping the Secret of Genetic Testing\n",
            "What is genetic risk ?\n",
            "Genetic risk refers more to your chance of inheriting a disorder or disease .\n",
            "People get certain disease because of genetic changes .\n",
            "How much a genetic change tells us about your chance of developing a disorder is not always clear .\n",
            "\n",
            "correction:\n",
            "Keeping the secret of Genetic Testing\n",
            "What is genetic risk ?\n",
            "Genetic risk refers more to your chance of inheriting a disorder or disease .\n",
            "People get certain diseases because of genetic changes .\n",
            "How much a genetic change tells us about your chance of developing a disorder is not always clear .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eS4nAov13BC2"
      },
      "source": [
        "##### Test #2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o0uzXThH3BC6",
        "colab": {}
      },
      "source": [
        "threshold = 3\n",
        "k = 20 "
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5nMxlIo73BDJ",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = jfleg_test_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZxTAF7SJ3BDQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e46d4bdc-aab3-47e6-f7e5-a8472bf167d6"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/jfleg/test/test.src'\n",
        "ref = ['/content/jfleg/test/test.ref0',\n",
        "       '/content/jfleg/test/test.ref1',\n",
        "       '/content/jfleg/test/test.ref2',\n",
        "       '/content/jfleg/test/test.ref3']\n",
        "#m2 = '...'\n",
        "hyp = write_file(suggestion, '/content/jfleg/test/test-En_BERT_test2_th=3,k=20.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "GLEU_score = calc_gleu(src, ref, hyp)\n",
        "print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "#M2_score = m2scorer(hyp, m2)\n",
        "#print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_BERT_JFLEG_test2_(th=3,k=20).txt'"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GLUE score = 44.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tNMGddH73BDY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "a8b0d949-092b-42b7-e206-759da2ced8b2"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[0:5], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[0:5], sep='\\n')"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "New and new technology has been introduced to the society .\n",
            "One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
            "Every person needs to know a bit about math , sciences , arts , literature and history in order to stand out in society .\n",
            "While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
            "Disadvantage is parking their car is very difficult .\n",
            "\n",
            "correction:\n",
            "New and new technology has been introduced to the city .\n",
            "One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries is outweigh and rise in motorization levels in the other countries .\n",
            "Every person needs to know a bit about math , sciences , arts , literature and history in order to stand out in society .\n",
            "While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour - for example due to entrance fees that would make the total package price overly expensive .\n",
            "Disadvantage , taking their car is very difficult .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WXQjYFt23Did"
      },
      "source": [
        "##### Test #3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0VEyX-dq3Dig",
        "colab": {}
      },
      "source": [
        "threshold = 5\n",
        "k = 15 "
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lpKaNneN3Dis",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = jfleg_test_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AVExBH113Di0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c70d9ad7-7df9-4b8c-9cb2-b890b2dabec5"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/jfleg/test/test.src'\n",
        "ref = ['/content/jfleg/test/test.ref0',\n",
        "       '/content/jfleg/test/test.ref1',\n",
        "       '/content/jfleg/test/test.ref2',\n",
        "       '/content/jfleg/test/test.ref3']\n",
        "#m2 = '...'\n",
        "hyp = write_file(suggestion, '/content/jfleg/test/test-En_BERT_test3_th=5,k=15.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "GLEU_score = calc_gleu(src, ref, hyp)\n",
        "print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "#M2_score = m2scorer(hyp, m2)\n",
        "#print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_BERT_JFLEG_test3_(th=5,k=15).txt'"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GLUE score = 39.13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cLohXnh53Di6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "91d0c5d3-816f-4519-d9f8-442987512ae5"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[0:5], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[0:5], sep='\\n')"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "New and new technology has been introduced to the society .\n",
            "One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
            "Every person needs to know a bit about math , sciences , arts , literature and history in order to stand out in society .\n",
            "While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
            "Disadvantage is parking their car is very difficult .\n",
            "\n",
            "correction:\n",
            "New and new technology has been introduced to the city .\n",
            "One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries is outweigh and rise in motorization levels in the other countries .\n",
            "Every person needs to know a bit about math , sciences , arts , literature and history in order to stand out in society .\n",
            "While the travel company will most likely show them some interesting sites in order for their customers to advocate for their journey to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour - for example due to extra fees that would make the total purchase price overly expensive .\n",
            "Disadvantage , taking their car is very difficult .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OGUsTtm9odi6"
      },
      "source": [
        "#### BEA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lWZJu0_h6ALl"
      },
      "source": [
        "##### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rMbc5_RW6ALr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "2b7d2473-ffe6-4a4a-fe42-c2914f5ed409"
      },
      "source": [
        "# hyp = src\n",
        "#---------------------------\n",
        "# file paths\n",
        "src = '/content/wi+locness/m2/ABCN.dev.gold.bea19.src'\n",
        "#ref = ...\n",
        "m2 = '/content/wi+locness/m2/ABCN.dev.gold.bea19.m2'\n",
        "hyp = '/content/wi+locness/m2/ABCN.dev.gold.bea19.src'\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 1.0000\n",
            "Recall      : 0.0000\n",
            "F_0.5       : 0.0000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DbEjMDt0odjB"
      },
      "source": [
        "##### Test #1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "21SRvB9fodjF",
        "colab": {}
      },
      "source": [
        "threshold = 2\n",
        "k = 10 "
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "atMD4AyZodjQ",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = bea_test_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0KxyOr6YodjY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "8a640a76-8baf-46df-cdb2-1e2458dbc059"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/wi+locness/m2/ABCN.dev.gold.bea19.src'\n",
        "#ref = ...\n",
        "m2 = '/content/wi+locness/m2/ABCN.dev.gold.bea19.m2'\n",
        "hyp = write_file(suggestion, '/content/wi+locness/m2/ABCN.dev.gold.bea19-En_BERT_test1_th=2,k=10.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_BERT_BEA_test1_(th=2,k=10).txt'"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 0.2083\n",
            "Recall      : 0.0950\n",
            "F_0.5       : 0.1682\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpyYKRkryGQP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "c767efc3-29ec-4d8d-ee8c-f90594d95e24"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[0:5], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[0:5], sep='\\n')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "It 's difficult answer at the question \" what are you going to do in the future ? \" if the only one who has to know it is in two minds .\n",
            "When I was younger I used to say that I wanted to be a teacher , a saleswoman and even a butcher .. I do n't know why .\n",
            "I would like to study Psychology because one day I would open my own psychology office and help people .\n",
            "It 's difficult because I 'll have to study hard and a lot , but I think that if you like a subject , you 'll study it easier .\n",
            "Maybe I 'll change my mind , maybe not .\n",
            "\n",
            "correction:\n",
            "It is difficult answer to the question \" what are you going to do in the future ? \" as the only one who has to know it is in two minds .\n",
            "When I was younger I used to say that I wanted to be a teacher , a saleswoman and even a butcher . I do not know why .\n",
            "I would like to study Psychology because one day I would open my own psychology office and help people .\n",
            "It is difficult because I will have to study hard and a lot , but I think that if you like a subject , you will study it easier .\n",
            "Maybe I will change my mind , maybe not .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iPxGT-4xzU8",
        "colab_type": "text"
      },
      "source": [
        "### 8.1.2 Using T5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP_jpvBtx09O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting tokenizer and model\n",
        "tokenizer = get_tokenizer('t5-large')\n",
        "model = get_model('t5-large')\n",
        "model.to(device);\n",
        "#---------------------------\n",
        "# hyperparameters\n",
        "edit_distance = get_distance_algorithm('damerau')"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r3t8LPGf4Pby"
      },
      "source": [
        "#### CoNLL-2013"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wnrAWqzP5EVs"
      },
      "source": [
        "##### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "af62y-pg5EVx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "95b60521-92c8-42a2-8906-60988d9f7b0e"
      },
      "source": [
        "# hyp = src\n",
        "#---------------------------\n",
        "# file paths\n",
        "src = '/content/release2.3.1/revised/data/official-preprocessed.src'\n",
        "#ref = ...\n",
        "m2 = '/content/release2.3.1/revised/data/official-preprocessed.m2'\n",
        "hyp = '/content/release2.3.1/revised/data/official-preprocessed.src'\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 1.0000\n",
            "Recall      : 0.0000\n",
            "F_0.5       : 0.0000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LqGIR2AK4Pb3"
      },
      "source": [
        "##### Test #1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bRj88m534Pb6",
        "colab": {}
      },
      "source": [
        "threshold = 2\n",
        "k = 10 \n",
        "b = 20"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pMQ1L9Xx4PcL",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = conll_2013_test_src\n",
        "suggestion = suggest_t5(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, b=b, threshold=threshold, device=device)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CEgJKXHA4PcU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "df6899d9-0271-4b2c-b3d1-923a6edff996"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/release2.3.1/revised/data/official-preprocessed.src'\n",
        "#ref = ...\n",
        "m2 = '/content/release2.3.1/revised/data/official-preprocessed.m2'\n",
        "hyp = write_file(suggestion, '/content/release2.3.1/revised/data/official-preprocessed-En-T5_test1_th=2,k=10,b=20.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_T5_CoNLL-2013_test1_(th=2,k=10,b=20).txt'"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 0.1248\n",
            "Recall      : 0.0767\n",
            "F_0.5       : 0.1109\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A8iiA39u4Pcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "1c728637-682d-498d-a152-e1c37936fff1"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[0:5], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[0:5], sep='\\n')"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "In modern digital world , electronic products are widely used in daily lives such as Smart phones , computers and etc .\n",
            "In work places , electronic devices such as computers are also inevitable to use to increase the productivity of the corporation .\n",
            "The convenience and high efficiency of using electronic products is being noticed by people worldwide .\n",
            "Some people started to think if electronic products can be further operated to more advanced utilization and replace human beings for better performances .\n",
            "Surveillance technology such as RFID ( radio-frequency identification ) is one type of examples that has currently been implemented .\n",
            "\n",
            "correction:\n",
            "In modern digital world , electronic products are widely used in daily lives such as smart phones , computers and etc .\n",
            "in work places , electronic devices such as computers are also inevitable to use to increase the productivity of the corporation .\n",
            "The convenience and high efficiency of using electronic products is being noticed by people worldwide .\n",
            "some people started to think of electronic products can be further operated to more advanced utilization and replace human being for better performance .\n",
            "Surveillance technology such as RFID ( radio-frequency identification ) is one type of examples that has currently been implemented in\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uLqDHhHk4Pco"
      },
      "source": [
        "##### Test #2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SBF50kce4Pcp",
        "colab": {}
      },
      "source": [
        "threshold = 3\n",
        "k = 20 \n",
        "b = 30"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ivjKs1zA4Pcv",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = conll_2013_test_src\n",
        "suggestion = suggest_t5(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, b=b, threshold=threshold, device=device)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wKd59Cgp4Pc3",
        "colab": {}
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/release2.3.1/revised/data/official-preprocessed.src'\n",
        "#ref = ...\n",
        "m2 = '/content/release2.3.1/revised/data/official-preprocessed.m2'\n",
        "hyp = write_file(suggestion, '/content/release2.3.1/revised/data/official-preprocessed-En-BERT_test2_th=3,k=20,b=30.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_BERT_CoNLL-2013_test2_(th=3,k=20,b=30).txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9DahELCM4Pc-",
        "colab": {}
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[0:5], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[0:5], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnlRGhayTJJs",
        "colab_type": "text"
      },
      "source": [
        "#### CoNLL-2014"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5T3nQclL8EGj",
        "colab_type": "text"
      },
      "source": [
        "##### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GMyOaIB-71KJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "3ae53c36-ea53-448f-bbb5-ad344b29ec88"
      },
      "source": [
        "# hyp = src\n",
        "#---------------------------\n",
        "# file paths\n",
        "src = '/content/conll14st-test-data/noalt/official-2014.1.src'\n",
        "#ref = ...\n",
        "m2 = '/content/conll14st-test-data/noalt/official-2014.1.m2'\n",
        "hyp = '/content/conll14st-test-data/noalt/official-2014.1.src'\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 1.0000\n",
            "Recall      : 0.0000\n",
            "F_0.5       : 0.0000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oELdNWiM73S7",
        "colab_type": "text"
      },
      "source": [
        "##### Test #1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JOtKdSKfRl_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "threshold = 3\n",
        "k = 30 \n",
        "b = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hufEhj7fLB00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = conll_2014_test_src\n",
        "suggestion = suggest_t5(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, b=b, threshold=threshold, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KQZpjxkg7pz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "c68f6739-5833-4b23-ebf8-02c3094b97d2"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/conll14st-test-data/noalt/official-2014.1.src'\n",
        "#ref = ...\n",
        "m2 = '/content/conll14st-test-data/noalt/official-2014.1.m2'\n",
        "hyp = write_file(suggestion, '/content/conll14st-test-data/noalt/official-2014.1-En_T5_test1_th=3,k=30,b=50.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_T5_CoNLL-2014_test1_(th=3,k=30,b=50).txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score = Precision   : 0.1283\n",
            "Recall      : 0.0841\n",
            "F_0.5       : 0.1161\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iXasKl_dMAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[1000:1005], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[1000:1005], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "in-0qblXfH_L"
      },
      "source": [
        "##### Test #2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE-Miol5fXbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "threshold = 1\n",
        "k = 30 \n",
        "b = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iSPkOYiefH_S",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = conll_2014_test_src\n",
        "suggestion = suggest_t5(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, b=b, threshold=threshold, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "292sAAO-2ckJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "67edc26d-dbfc-41b2-ac4e-8a8be2e3e33d"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/conll14st-test-data/noalt/official-2014.1.src'\n",
        "#ref = ...\n",
        "m2 = '/content/conll14st-test-data/noalt/official-2014.1.m2'\n",
        "hyp = write_file(suggestion, '/content/conll14st-test-data/noalt/official-2014.1-En_T5_test2_th=1,k=30,b=50.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_T5_CoNLL-2014_test2_(th=1,k=30,b=50).txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 0.1352\n",
            "Recall      : 0.0582\n",
            "F_0.5       : 0.1069\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eS7ezwMJ0iqK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "c4e26f70-d4ff-4d37-c9d3-646ab3ff37c0"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[1000:1005], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[1000:1005], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "Undeniable , it becomes more addicting when we spend more time busy socialising and interacting virtually .\n",
            "We spend majority of our time on sites like Facebook , Twitter and it affects our daily work productivity and performance .\n",
            "In corporate world , employers often block social media network to prevent employees to spend their office time on their personal leisure than concentrating on their work .\n",
            "Using text-messaging language as an informal way of communicating on social media network also brings in a bad impact for us in a long term .\n",
            "The more time we spend on these sites , the lesser time we spend on face-to-face interacting with one another .\n",
            "\n",
            "correction:\n",
            "Undeniable , it becomes more addicting when we spend more time busy socialising and interacting virtually .\n",
            "We spend majority of our time on sites like Facebook , Twitter and it affect our daily work productivity and performance .\n",
            "In corporate world , employers often block social media networks to prevent employees to spend their office time on their personal leisure than concentrating on their work .\n",
            "using text-messaging language as an informal way of communicating on social media network also brings in  bad impact for us in  long term .\n",
            "The more time we spend on these sites , the lesser time we spend on face-to-face interacting with one another .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ukMsJdSh3q0K"
      },
      "source": [
        "##### Test #3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EY_u2P7I3q0P",
        "colab": {}
      },
      "source": [
        "threshold = 2\n",
        "k = 30 \n",
        "b = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S07xPgNV3q0e",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = conll_2014_test_src\n",
        "suggestion = suggest_t5(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, b=b, threshold=threshold, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-M3wcOLs3q0u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "69252994-eaaa-4797-e0e7-a02896f4d8e9"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/conll14st-test-data/noalt/official-2014.1.src'\n",
        "#ref = ...\n",
        "m2 = '/content/conll14st-test-data/noalt/official-2014.1.m2'\n",
        "hyp = write_file(suggestion, '/content/conll14st-test-data/noalt/official-2014.1-En_T5_test3_th=2,k=30,b=50.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_T5_CoNLL-2014_test3_(th=2,k=30,b=50).txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 0.1380\n",
            "Recall      : 0.0715\n",
            "F_0.5       : 0.1163\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZKaAh4kX3q02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "b3451c05-8076-4046-86c2-95ceea7ec40e"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[1000:1005], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[1000:1005], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "Undeniable , it becomes more addicting when we spend more time busy socialising and interacting virtually .\n",
            "We spend majority of our time on sites like Facebook , Twitter and it affects our daily work productivity and performance .\n",
            "In corporate world , employers often block social media network to prevent employees to spend their office time on their personal leisure than concentrating on their work .\n",
            "Using text-messaging language as an informal way of communicating on social media network also brings in a bad impact for us in a long term .\n",
            "The more time we spend on these sites , the lesser time we spend on face-to-face interacting with one another .\n",
            "\n",
            "correction:\n",
            "Undeniable , it becomes more addicting when we spend more time just socialising and interacting virtually .\n",
            "We spend majority of our time on sites like Facebook , Twitter and it affect our daily work productivity and performance .\n",
            "In corporate world , employers often block social media networks to prevent employees to spend their office time on their personal leisure than concentrating on their work .\n",
            "using text-messaging language as an informal way of communicating on social media network also brings in  big impact for us in  long term .\n",
            "The more time we spend on these sites , the lesser time we spend on face-to-face interaction with one another .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-W81bcuHL8dK"
      },
      "source": [
        "##### Test #4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mdhDPg5fL8dN",
        "colab": {}
      },
      "source": [
        "threshold = 2\n",
        "k = 10 \n",
        "b = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Mf8G_veWL8db",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = conll_2014_test_src\n",
        "suggestion = suggest_t5(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, b=b, threshold=threshold, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EyrqSkP5L8dk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "e1556ba4-579b-4b1b-a126-ddb6208d4ea0"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/conll14st-test-data/noalt/official-2014.1.src'\n",
        "#ref = ...\n",
        "m2 = '/content/conll14st-test-data/noalt/official-2014.1.m2'\n",
        "hyp = write_file(suggestion, '/content/conll14st-test-data/noalt/official-2014.1-En_T5_test4_th=2,k=10,b=20.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_T5_CoNLL-2014_test4_(th=2,k=10,b=20).txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 0.1298\n",
            "Recall      : 0.0841\n",
            "F_0.5       : 0.1171\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zjj-pVokL8dt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "42f92fa3-521a-4afc-c8de-d9060c249221"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[1000:1005], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[1000:1005], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "Undeniable , it becomes more addicting when we spend more time busy socialising and interacting virtually .\n",
            "We spend majority of our time on sites like Facebook , Twitter and it affects our daily work productivity and performance .\n",
            "In corporate world , employers often block social media network to prevent employees to spend their office time on their personal leisure than concentrating on their work .\n",
            "Using text-messaging language as an informal way of communicating on social media network also brings in a bad impact for us in a long term .\n",
            "The more time we spend on these sites , the lesser time we spend on face-to-face interacting with one another .\n",
            "\n",
            "correction:\n",
            "Undeniable , it becomes more addicting when we spend more time busy socialising and interacting virtually .\n",
            "We spend majority of our time on sites like Facebook , Twitter and it affect our daily work productivity and performance .\n",
            "In corporate world , employers often block social media network to prevent employees to spend their office time on their personal leisure than concentrating on their work .\n",
            "using text-messaging language as an informal way of communicating on social media network also brings in  big impact for  in  long term .\n",
            "The more time we spend on these sites , the lesser time we spend on face-to-face interaction with one another .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0SDT8CQmpRaq"
      },
      "source": [
        "##### Test #5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z_OmyHqJpRay",
        "colab": {}
      },
      "source": [
        "threshold = 2\n",
        "k = 10 \n",
        "b = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g-luQRr-pRa_",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = conll_2014_test_src\n",
        "suggestion = suggest_t5(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, b=b, threshold=threshold, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmmZhH4lpRbH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "2298a3c1-0f37-47c5-cd22-e97bdc0fe0f5"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/conll14st-test-data/noalt/official-2014.1.src'\n",
        "#ref = ...\n",
        "m2 = '/content/conll14st-test-data/noalt/official-2014.1.m2'\n",
        "hyp = write_file(suggestion, '/content/conll14st-test-data/noalt/official-2014.1-En_T5_test5_th=2,k=10,b=10.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_T5_CoNLL-2014_test5_(th=2,k=10,b=10).txt'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 0.1298\n",
            "Recall      : 0.0841\n",
            "F_0.5       : 0.1171\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RQtBuSVMpRbP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "67bf6ad3-2a9d-4f16-fd2c-1f23f1ad6adc"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[1000:1005], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[1000:1005], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "Undeniable , it becomes more addicting when we spend more time busy socialising and interacting virtually .\n",
            "We spend majority of our time on sites like Facebook , Twitter and it affects our daily work productivity and performance .\n",
            "In corporate world , employers often block social media network to prevent employees to spend their office time on their personal leisure than concentrating on their work .\n",
            "Using text-messaging language as an informal way of communicating on social media network also brings in a bad impact for us in a long term .\n",
            "The more time we spend on these sites , the lesser time we spend on face-to-face interacting with one another .\n",
            "\n",
            "correction:\n",
            "Undeniable , it becomes more addicting when we spend more time busy socialising and interacting virtually .\n",
            "We spend majority of our time on sites like Facebook , Twitter and it affect our daily work productivity and performance .\n",
            "In corporate world , employers often block social media network to prevent employees to spend their office time on their personal leisure than concentrating on their work .\n",
            "using text-messaging language as an informal way of communicating on social media network also brings in  big impact for  in  long term .\n",
            "The more time we spend on these sites , the lesser time we spend on face-to-face interaction with one another .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpPzkXoM0zRH",
        "colab_type": "text"
      },
      "source": [
        "#### JFLEG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bg8iBD-R7Fjv"
      },
      "source": [
        "##### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U3-WOWqw7Fj2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "1673795c-ec98-416a-d954-42c0abcce265"
      },
      "source": [
        "# hyp = src\n",
        "#---------------------------\n",
        "# source file\n",
        "src = 'jfleg/test/test.src'\n",
        "# reference file\n",
        "ref = ['jfleg/test/test.ref0',\n",
        "       'jfleg/test/test.ref1',\n",
        "       'jfleg/test/test.ref2',\n",
        "       'jfleg/test/test.ref3']\n",
        "# hypothesis file\n",
        "hyp = 'jfleg/test/test.src'\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "GLEU_score = calc_gleu(src, ref, hyp)\n",
        "print(f'GLUE score = {GLEU_score:.2f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GLUE score = 40.47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xwRusu3r7FkF"
      },
      "source": [
        "##### Test #1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ltI8Nu3Y7FkI",
        "colab": {}
      },
      "source": [
        "threshold = 2\n",
        "k = 10 \n",
        "b = 20"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jLN5kSmN7FkZ",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = jfleg_test_src\n",
        "suggestion = suggest_t5(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, b=b, threshold=threshold, device=device)"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FFfj_OLq7Fkj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e28ed9ac-e02b-4024-a58e-46bee8c1b6bf"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/jfleg/test/test.src'\n",
        "ref = ['/content/jfleg/test/test.ref0',\n",
        "       '/content/jfleg/test/test.ref1',\n",
        "       '/content/jfleg/test/test.ref2',\n",
        "       '/content/jfleg/test/test.ref3']\n",
        "#m2 = '...'\n",
        "hyp = write_file(suggestion, '/content/jfleg/test/test-En_T5_test1_th=2,k=10,b=20.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "GLEU_score = calc_gleu(src, ref, hyp)\n",
        "print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "#M2_score = m2scorer(hyp, m2)\n",
        "#print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_T5_JFLEG_test1_(th=2,k=10,b=20).txt'"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GLUE score = 40.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MV7E_7WM7Fks",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "49937aa2-ea9a-462a-f1ca-7712e4fc78e3"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[0:5], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[0:5], sep='\\n')"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "New and new technology has been introduced to the society .\n",
            "One possible outcome is that an environmentally-induced reduction in motorization levels in the richer countries will outweigh any rise in motorization levels in the poorer countries .\n",
            "Every person needs to know a bit about math , sciences , arts , literature and history in order to stand out in society .\n",
            "While the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour -- for example due to entrance fees that would make the total package price overly expensive .\n",
            "Disadvantage is parking their car is very difficult .\n",
            "\n",
            "correction:\n",
            "New and new technology has been introduced to the society .\n",
            "One possible outcome is that an environmentally-induced reduction in motorization levels in the riches countries will outweigh any rise in motorization levels in the poor countries .\n",
            "every person needs to know  bit about math , sciences , arts , literature and history in order to stand out in society .\n",
            "while the travel company will most likely show them some interesting sites in order for their customers to advertise for their company to their family and friends , it is highly unlikely , that the company will tell about the sites that were not included in the tour  for example due to entrance fees that would make the total package price very expensive .\n",
            "Disadvantage is parking the car is very difficult .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QOaoGRqP7Fk1"
      },
      "source": [
        "##### Test #2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OFMfdQQy7Fk2",
        "colab": {}
      },
      "source": [
        "threshold = 3\n",
        "k = 20 \n",
        "b = 30"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_IWfyzIJ7Fk9",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = jfleg_test_src\n",
        "suggestion = suggest_t5(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, b=b, threshold=threshold, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N-xK8cmZ7FlD",
        "colab": {}
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/jfleg/test/test.src'\n",
        "ref = ['/content/jfleg/test/test.ref0',\n",
        "       '/content/jfleg/test/test.ref1',\n",
        "       '/content/jfleg/test/test.ref2',\n",
        "       '/content/jfleg/test/test.ref3']\n",
        "#m2 = '...'\n",
        "hyp = write_file(suggestion, '/content/jfleg/test/test-En_T5_test2_th=3,k=20,b=30.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "GLEU_score = calc_gleu(src, ref, hyp)\n",
        "print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "#M2_score = m2scorer(hyp, m2)\n",
        "#print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_T5_JFLEG_test2_(th=3,k=20,b=30).txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PrAb_uka7FlJ",
        "colab": {}
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[0:5], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[0:5], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ng8yBfxY7FlR"
      },
      "source": [
        "##### Test #3\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P_YBPVz17FlS",
        "colab": {}
      },
      "source": [
        "threshold = 5\n",
        "k = 15 \n",
        "b = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "suPPIcEx7FlY",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = jfleg_test_src\n",
        "suggestion = suggest_t5(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, b=b, threshold=threshold, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sNUFx4UR7Flc",
        "colab": {}
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/jfleg/test/test.src'\n",
        "ref = ['/content/jfleg/test/test.ref0',\n",
        "       '/content/jfleg/test/test.ref1',\n",
        "       '/content/jfleg/test/test.ref2',\n",
        "       '/content/jfleg/test/test.ref3']\n",
        "#m2 = '...'\n",
        "hyp = write_file(suggestion, '/content/jfleg/test/test-En_T5_test3_th=5,k=15,b=30.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "GLEU_score = calc_gleu(src, ref, hyp)\n",
        "print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "#M2_score = m2scorer(hyp, m2)\n",
        "#print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_T5_JFLEG_test3_(th=5,k=15,b=30).txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yjSv-y-c7Flm",
        "colab": {}
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[0:5], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[0:5], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGuxtK2CTEbN",
        "colab_type": "text"
      },
      "source": [
        "#### BEA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f_TZWqQG8N7g"
      },
      "source": [
        "##### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B0XO8X_r8N7p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "ac8d0df0-8f7d-4ed7-c07e-1abfa9f974b6"
      },
      "source": [
        "# hyp = src\n",
        "#---------------------------\n",
        "# file paths\n",
        "src = '/content/wi+locness/m2/ABCN.dev.gold.bea19.src'\n",
        "#ref = ...\n",
        "m2 = '/content/wi+locness/m2/ABCN.dev.gold.bea19.m2'\n",
        "hyp = '/content/wi+locness/m2/ABCN.dev.gold.bea19.src'\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 1.0000\n",
            "Recall      : 0.0000\n",
            "F_0.5       : 0.0000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O42bLeHA8N76"
      },
      "source": [
        "##### Test #1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H1qPCL-98N78",
        "colab": {}
      },
      "source": [
        "threshold = 2\n",
        "k = 10 \n",
        "b = 20"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O70LB82H8N8J",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = bea_test_src\n",
        "suggestion = suggest_t5(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, b=b, threshold=threshold, device=device)"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dZpaZ5dO8N8S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "9590a8cb-7e89-4a25-d9b1-93122bb4e2cd"
      },
      "source": [
        "# calculate scores\n",
        "src = '/content/wi+locness/m2/ABCN.dev.gold.bea19.src'\n",
        "#ref = ...\n",
        "m2 = '/content/wi+locness/m2/ABCN.dev.gold.bea19.m2'\n",
        "hyp = write_file(suggestion, '/content/wi+locness/m2/ABCN.dev.gold.bea19-En_T5_test1_th=2,k=10,b=20.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "#GLEU_score = calc_gleu(src, ref, hyp)\n",
        "#print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "M2_score = m2scorer(hyp, m2)\n",
        "print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/En_T5_BEA_test1_(th=2,k=10,b=20).txt'"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "M^2 score\n",
            "----------\n",
            "Precision   : 0.1068\n",
            "Recall      : 0.0989\n",
            "F_0.5       : 0.1051\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AGA2zotM8N8Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        },
        "outputId": "f64fde4d-1d59-46bc-bdb4-80fbbd891aec"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[0:5], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[0:5], sep='\\n')"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "It 's difficult answer at the question \" what are you going to do in the future ? \" if the only one who has to know it is in two minds .\n",
            "When I was younger I used to say that I wanted to be a teacher , a saleswoman and even a butcher .. I do n't know why .\n",
            "I would like to study Psychology because one day I would open my own psychology office and help people .\n",
            "It 's difficult because I 'll have to study hard and a lot , but I think that if you like a subject , you 'll study it easier .\n",
            "Maybe I 'll change my mind , maybe not .\n",
            "\n",
            "correction:\n",
            "It is difficult answer to the question \" what are you going to do in the future ? \"  the only one who has to know it is in the minds .\n",
            "When I was younger I used to say that I wanted to be  teacher , a saleswoman and even  butcher . I do not know why .\n",
            "I would like to study Psychology because one day I would open my own psychology office and help people in\n",
            "It ' difficult because I 'll have to study hard and I lot , but I think that it you like  subject , you will study it easier .\n",
            "Maybe I will change my mind , maybe not .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5slfc1xRQ2TU",
        "colab_type": "text"
      },
      "source": [
        "## 8.2 Portuguese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAzmHzBbxvLL",
        "colab_type": "text"
      },
      "source": [
        "### 8.2.1 Using BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lahN41d0x5SG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# getting tokenizer and model\n",
        "tokenizer = get_tokenizer('neuralmind/bert-large-portuguese-cased')\n",
        "model = get_model('neuralmind/bert-large-portuguese-cased')\n",
        "model.to(device);\n",
        "#---------------------------\n",
        "# hyperparameters\n",
        "k = 10\n",
        "edit_distance = get_distance_algorithm('damerau')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjW0qnxmx8Ch",
        "colab_type": "text"
      },
      "source": [
        "#### ReGRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlZe-bjVNXjn",
        "colab_type": "text"
      },
      "source": [
        "##### Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvkNGknBNaE7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "0a537342-60cc-452c-c20d-22a1a89dbdf1"
      },
      "source": [
        "# hyp = src\n",
        "#---------------------------\n",
        "# file paths\n",
        "src = regra_src_file\n",
        "ref = regra_ref_file\n",
        "#m2 = ...\n",
        "hyp = regra_src_file\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "GLEU_score = calc_gleu(src, [ref], hyp)\n",
        "print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "#M2_score = m2scorer(hyp, m2)\n",
        "#print(f'M^2 score\\n----------\\n{M2_score}')"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is one reference. NOTE: GLEU is not computing the confidence interval.\n",
            "GLUE score = 36.99\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b2nnaJgQ9DFR"
      },
      "source": [
        "##### Test #1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w0G8H0TN9DFV",
        "colab": {}
      },
      "source": [
        "threshold = 2\n",
        "k = 10 "
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "otXBTQ_M9DFj",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = regra_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LalCOmim9DFz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "outputId": "73aa32b9-c7f5-4212-b392-0835ebabccc9"
      },
      "source": [
        "# calculate scores\n",
        "src = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/src.txt'\n",
        "ref = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/ref.txt'\n",
        "#m2 = '...'\n",
        "hyp = write_file(suggestion, '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/Pt_T5_test1_th=2,k=10.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "GLEU_score = calc_gleu(src, [ref], hyp)\n",
        "print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "#M2_score = m2scorer(hyp, m2)\n",
        "#print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/Pt_T5_ReGRA_test1_(th=2,k=10).txt'"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is one reference. NOTE: GLEU is not computing the confidence interval.\n",
            "GLUE score = 31.81\n",
            "cp: target '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/Pt_T5_ReGRA_test1_(th=2,k=10).txt' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZIPLnxTTUKM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "e05a6b0e-3702-46fb-9bc0-159b8c3c09c7"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[1000:1005], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[1000:1005], sep='\\n')"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "Uma delegação de padeiros vem prestar seu apoio as mulheres dos grevistas.\n",
            "Uma era ítala-brasileira.\n",
            "Uma frota de navios norte-americanos se dirigiste ao Mar Mediterrâneo.\n",
            "Uma noite, muito a escondida, o padre saiu.\n",
            "Uma palavra, um gesto, um olhar bastavam para eu te seguir.\n",
            "\n",
            "correction:\n",
            "Uma delegação de padeiros vem prestar seu apoio as mulheres do grevistas.\n",
            "Uma pena ítala-brasileira.\n",
            "Uma frota de navios norte-americanos e dirigiste ao mar Mediterrâneo\n",
            "Ela noite, muito boa escondida, o padre saiu.\n",
            "Uma palavra uma gesto, um olhar bastavam para que ter seguir.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bGX3CvnmbcOp"
      },
      "source": [
        "##### Test #2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Stkrfz7V-ddW",
        "colab": {}
      },
      "source": [
        "threshold = 2\n",
        "k = 2"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QtfSqfBf-ddo",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = regra_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ig2zLt1Q-dd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "outputId": "6bf4ca1c-017b-4a7b-c67f-49058644bf9b"
      },
      "source": [
        "# calculate scores\n",
        "src = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/src.txt'\n",
        "ref = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/ref.txt'\n",
        "#m2 = '...'\n",
        "hyp = write_file(suggestion, '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/Pt_T5_test2_th=1,k=30.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "GLEU_score = calc_gleu(src, [ref], hyp)\n",
        "print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "#M2_score = m2scorer(hyp, m2)\n",
        "#print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/Pt_T5_ReGRA_test2_(th=1,k=30).txt'"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is one reference. NOTE: GLEU is not computing the confidence interval.\n",
            "GLUE score = 33.90\n",
            "cp: target '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/Pt_T5_ReGRA_test2_(th=1,k=30).txt' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A5qovT6W-deC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "c7fd8938-8504-4a3b-d269-ef8fd4890b76"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[1000:1005], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[1000:1005], sep='\\n')"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "Uma delegação de padeiros vem prestar seu apoio as mulheres dos grevistas.\n",
            "Uma era ítala-brasileira.\n",
            "Uma frota de navios norte-americanos se dirigiste ao Mar Mediterrâneo.\n",
            "Uma noite, muito a escondida, o padre saiu.\n",
            "Uma palavra, um gesto, um olhar bastavam para eu te seguir.\n",
            "\n",
            "correction:\n",
            "Uma delegação de padeiros veio prestar seu apoio as mulheres do grevistas.\n",
            "Uma pena ítala-brasileira.\n",
            "Uma frota de navios norte-americanos e dirigiste no mar Mediterrâneo\n",
            "Uma noite, muito boa escondida, do padre saiu.\n",
            "Um palavra um gesto, um olhar bastavam para eu ver seguir.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qf4mQfFObioa"
      },
      "source": [
        "##### Test #3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wuzi5zLv-q47",
        "colab": {}
      },
      "source": [
        "threshold = 3\n",
        "k = 15 "
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FACn8bvh-q5R",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = regra_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oA6nIj_r-q5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "outputId": "61471d88-ffd4-4d92-ff72-0363ae3f84b6"
      },
      "source": [
        "# calculate scores\n",
        "src = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/src.txt'\n",
        "ref = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/ref.txt'\n",
        "#m2 = '...'\n",
        "hyp = write_file(suggestion, '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/Pt_T5_test3_th=3,k=15.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "GLEU_score = calc_gleu(src, [ref], hyp)\n",
        "print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "#M2_score = m2scorer(hyp, m2)\n",
        "#print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/Pt_T5_ReGRA_test3_(th=3,k=15).txt'"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is one reference. NOTE: GLEU is not computing the confidence interval.\n",
            "GLUE score = 26.52\n",
            "cp: target '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/Pt_T5_ReGRA_test3_(th=3,k=15).txt' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AZFp2yO_-q5n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "5479c944-0084-49ae-c0e3-29f4f7e2ef62"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[1000:1005], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[1000:1005], sep='\\n')"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "Uma delegação de padeiros vem prestar seu apoio as mulheres dos grevistas.\n",
            "Uma era ítala-brasileira.\n",
            "Uma frota de navios norte-americanos se dirigiste ao Mar Mediterrâneo.\n",
            "Uma noite, muito a escondida, o padre saiu.\n",
            "Uma palavra, um gesto, um olhar bastavam para eu te seguir.\n",
            "\n",
            "correction:\n",
            "Uma delegação de padeiros vem prestar seu apoio as mulheres do grevistas.\n",
            "Já pena ítala-brasileira.\n",
            "Uma frota de navios norte-americanos e dirige ao mar Mediterrâneo\n",
            "Ela noite, muito boa escondida, o de saiu.\n",
            "Uma palavra um gesto, um olhar bastavam para que ter seguir.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kgkOxeNL-gI5"
      },
      "source": [
        "##### Test #4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1j31Q9RP-gI9",
        "colab": {}
      },
      "source": [
        "threshold = 1\n",
        "k = 2"
      ],
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y4kaQwSg-gJQ",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = regra_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "niO0DdV3-gJX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "outputId": "73e64fdb-3af3-4466-ee8f-965f6ad48d76"
      },
      "source": [
        "# calculate scores\n",
        "src = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/src.txt'\n",
        "ref = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/ref.txt'\n",
        "#m2 = '...'\n",
        "hyp = write_file(suggestion, '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/Pt_T5_test4_th=1,k=2.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "GLEU_score = calc_gleu(src, [ref], hyp)\n",
        "print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "#M2_score = m2scorer(hyp, m2)\n",
        "#print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/Pt_T5_ReGRA_test4_(th=1,k=2).txt'"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is one reference. NOTE: GLEU is not computing the confidence interval.\n",
            "GLUE score = 38.29\n",
            "cp: target '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/Pt_T5_ReGRA_test4_(th=1,k=2).txt' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rofAtKs-gJi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "5479c944-0084-49ae-c0e3-29f4f7e2ef62"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[1000:1005], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[1000:1005], sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "Uma delegação de padeiros vem prestar seu apoio as mulheres dos grevistas.\n",
            "Uma era ítala-brasileira.\n",
            "Uma frota de navios norte-americanos se dirigiste ao Mar Mediterrâneo.\n",
            "Uma noite, muito a escondida, o padre saiu.\n",
            "Uma palavra, um gesto, um olhar bastavam para eu te seguir.\n",
            "\n",
            "correction:\n",
            "Uma delegação de padeiros vem prestar seu apoio as mulheres do grevistas.\n",
            "Já pena ítala-brasileira.\n",
            "Uma frota de navios norte-americanos e dirige ao mar Mediterrâneo\n",
            "Ela noite, muito boa escondida, o de saiu.\n",
            "Uma palavra um gesto, um olhar bastavam para que ter seguir.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "caJFe9_2-r8h"
      },
      "source": [
        "##### Test #5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rJ2xVR5d-r8n",
        "colab": {}
      },
      "source": [
        "threshold = 2\n",
        "k = 1"
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y_QoY7O0-r82",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = regra_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mc7kk668-r9B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "outputId": "e6d6cece-ba1a-4ee7-a42a-b8b07b55cf3c"
      },
      "source": [
        "# calculate scores\n",
        "src = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/src.txt'\n",
        "ref = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/ref.txt'\n",
        "#m2 = '...'\n",
        "hyp = write_file(suggestion, '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/Pt_T5_test5_th=2,k=1.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "GLEU_score = calc_gleu(src, [ref], hyp)\n",
        "print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "#M2_score = m2scorer(hyp, m2)\n",
        "#print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/Pt_T5_ReGRA_test5_(th=2,k=1).txt'"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is one reference. NOTE: GLEU is not computing the confidence interval.\n",
            "GLUE score = 36.06\n",
            "cp: target '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/Pt_T5_ReGRA_test5_(th=2,k=1).txt' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "htLR-1UO-r9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "0cebbe5a-ae0c-454f-bcc8-0c6222268dcc"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[1000:1005], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[1000:1005], sep='\\n')"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "Uma delegação de padeiros vem prestar seu apoio as mulheres dos grevistas.\n",
            "Uma era ítala-brasileira.\n",
            "Uma frota de navios norte-americanos se dirigiste ao Mar Mediterrâneo.\n",
            "Uma noite, muito a escondida, o padre saiu.\n",
            "Uma palavra, um gesto, um olhar bastavam para eu te seguir.\n",
            "\n",
            "correction:\n",
            "Uma delegação de padeiros veio prestar seu apoio às mulheres do grevistas.\n",
            "Uma pena ítala-brasileira.\n",
            "Uma frota de navios norte-americanos se dirigiste ao mar Mediterrâneo.\n",
            "Uma noite, muito boa escondida, do padre saiu.\n",
            "Um palavra um gesto, um olhar bastavam para eu te seguir.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kv8ixISwAKE1"
      },
      "source": [
        "##### Test #6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aJe9NlSHAKE8",
        "colab": {}
      },
      "source": [
        "threshold = 2\n",
        "k = 3"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YnnwzS-9AKFL",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = regra_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zutj4aa-AKFY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "outputId": "652d8a77-f09b-48c5-f0b6-e935e16a015d"
      },
      "source": [
        "# calculate scores\n",
        "src = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/src.txt'\n",
        "ref = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/ref.txt'\n",
        "#m2 = '...'\n",
        "hyp = write_file(suggestion, '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/Pt_T5_test6_th=2,k=3.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "GLEU_score = calc_gleu(src, [ref], hyp)\n",
        "print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "#M2_score = m2scorer(hyp, m2)\n",
        "#print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/Pt_T5_ReGRA_test6_(th=2,k=3).txt'"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is one reference. NOTE: GLEU is not computing the confidence interval.\n",
            "GLUE score = 32.81\n",
            "cp: target '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/Pt_T5_ReGRA_test6_(th=2,k=3).txt' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XKL9YgU0AKFh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "808e69d1-d23a-4d64-8e02-157832a1e964"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[1000:1005], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[1000:1005], sep='\\n')"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "Uma delegação de padeiros vem prestar seu apoio as mulheres dos grevistas.\n",
            "Uma era ítala-brasileira.\n",
            "Uma frota de navios norte-americanos se dirigiste ao Mar Mediterrâneo.\n",
            "Uma noite, muito a escondida, o padre saiu.\n",
            "Uma palavra, um gesto, um olhar bastavam para eu te seguir.\n",
            "\n",
            "correction:\n",
            "Uma delegação de padeiros veio prestar seu apoio as mulheres do grevistas.\n",
            "Uma pena ítala-brasileira.\n",
            "Uma frota de navios norte-americanos e dirigiste ao mar Mediterrâneo\n",
            "Uma noite, muito boa escondida, do padre saiu.\n",
            "Um palavra um gesto, um olhar bastavam para que ver seguir.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DV6da0NYC__1"
      },
      "source": [
        "##### Test #7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jyeVkCHYC__5",
        "colab": {}
      },
      "source": [
        "threshold = 3\n",
        "k = 2"
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CziMVg36DAAD",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = regra_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kd_BK3VXDAAQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "outputId": "4a784ef1-825c-4f5a-9a0c-1d4f94ee1a6d"
      },
      "source": [
        "# calculate scores\n",
        "src = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/src.txt'\n",
        "ref = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/ref.txt'\n",
        "#m2 = '...'\n",
        "hyp = write_file(suggestion, '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/Pt_T5_test7_th=3,k=2.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "GLEU_score = calc_gleu(src, [ref], hyp)\n",
        "print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "#M2_score = m2scorer(hyp, m2)\n",
        "#print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/Pt_T5_ReGRA_test7_(th=3,k=2).txt'"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is one reference. NOTE: GLEU is not computing the confidence interval.\n",
            "GLUE score = 27.61\n",
            "cp: target '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/Pt_T5_ReGRA_test7_(th=3,k=2).txt' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TsrPaVzEDAAb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "e688f64d-50e7-417b-a470-230096e93c81"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[1000:1005], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[1000:1005], sep='\\n')"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "Uma delegação de padeiros vem prestar seu apoio as mulheres dos grevistas.\n",
            "Uma era ítala-brasileira.\n",
            "Uma frota de navios norte-americanos se dirigiste ao Mar Mediterrâneo.\n",
            "Uma noite, muito a escondida, o padre saiu.\n",
            "Uma palavra, um gesto, um olhar bastavam para eu te seguir.\n",
            "\n",
            "correction:\n",
            "Uma delegação de padeiros veio prestar seu apoio as mulheres do grevistas.\n",
            "Já pena ítala-brasileira.\n",
            "Uma frota de navios norte-americanos e dirige no mar Mediterrâneo\n",
            "Ele noite, muito boa escondida, do padre saiu.\n",
            "Um palavra um gesto, um olhar bastavam para quem ver seguir.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uw88AseKDy_J"
      },
      "source": [
        "##### Test #8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0rZzLN1QDy_O",
        "colab": {}
      },
      "source": [
        "threshold = 1\n",
        "k = 3"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i1KdUpCPDy_a",
        "colab": {}
      },
      "source": [
        "# suggestion\n",
        "sentence = regra_src\n",
        "suggestion = suggest_bert(sentence, model=model, tokenizer=tokenizer, distance=edit_distance, split=True, k=k, threshold=threshold, device=device)"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "03f3tYY1Dy_i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "outputId": "a2577f97-9e6b-46bc-dfb2-917775b6f9d9"
      },
      "source": [
        "# calculate scores\n",
        "src = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/src.txt'\n",
        "ref = '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/ref.txt'\n",
        "#m2 = '...'\n",
        "hyp = write_file(suggestion, '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/ReGRA/Pt_T5_test8_th=1,k=3.cor')\n",
        "#---------------------------\n",
        "# GLEU score\n",
        "GLEU_score = calc_gleu(src, [ref], hyp)\n",
        "print(f'GLUE score = {GLEU_score:.2f}')\n",
        "#---------------------------\n",
        "# M^2 score\n",
        "#M2_score = m2scorer(hyp, m2)\n",
        "#print(f'M^2 score\\n----------\\n{M2_score}')\n",
        "#---------------------------\n",
        "# save output\n",
        "!cp $hyp '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/Pt_T5_ReGRA_test8_(th=1,k=3).txt'"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is one reference. NOTE: GLEU is not computing the confidence interval.\n",
            "GLUE score = 37.14\n",
            "cp: target '/gdrive/My Drive/Colab Notebooks/IA376E/Final Project/Corrections/Pt_T5_ReGRA_test8_(th=1,k=3).txt' is not a directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yBghzzLODy_r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "fd90c5ec-416d-4f5a-ca9e-f942de5dca60"
      },
      "source": [
        "# original\n",
        "original = read_file(src)\n",
        "print('original:', *original[1000:1005], sep='\\n', end='\\n'*2)\n",
        "#---------------------------\n",
        "# correction\n",
        "corrections = read_file(hyp)\n",
        "print('correction:', *corrections[1000:1005], sep='\\n')"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original:\n",
            "Uma delegação de padeiros vem prestar seu apoio as mulheres dos grevistas.\n",
            "Uma era ítala-brasileira.\n",
            "Uma frota de navios norte-americanos se dirigiste ao Mar Mediterrâneo.\n",
            "Uma noite, muito a escondida, o padre saiu.\n",
            "Uma palavra, um gesto, um olhar bastavam para eu te seguir.\n",
            "\n",
            "correction:\n",
            "Uma delegação de padeiros vem prestar seu apoio as mulheres do grevistas.\n",
            "Uma era ítala-brasileira.\n",
            "Uma frota de navios norte-americanos e dirigiste ao mar Mediterrâneo\n",
            "Uma noite, muito a escondida, do padre saiu.\n",
            "Um palavra um gesto, um olhar bastavam para eu te seguir.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr1G4D6Bx72j",
        "colab_type": "text"
      },
      "source": [
        "## 8.2.2 Using T5 (TODO)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSv2_NWFx7tl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # getting tokenizer and model\n",
        "# tokenizer = get_tokenizer('t5-large')\n",
        "# model = get_model('.../t5-large-portuguese')\n",
        "# model.to(device);\n",
        "# #---------------------------\n",
        "# # hyperparameters\n",
        "# k = 30\n",
        "# b = 50\n",
        "# edit_distance = get_distance_algorithm('damerau')\n",
        "# threshold = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FamgumGXx7ip",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#\n",
        "#\n",
        "# TODO after Portuguese T5 release\n",
        "#\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRxEhLtQGMSm",
        "colab_type": "text"
      },
      "source": [
        "# 9. Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOxyVGC1GOUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIrP-AwgwDIA",
        "colab_type": "text"
      },
      "source": [
        "# 10. Conclusion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIsR45XswHQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5eLIfndsxo2",
        "colab_type": "text"
      },
      "source": [
        "# 11. Appendix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmCc9YnJPU_i",
        "colab_type": "text"
      },
      "source": [
        "## 11.1 Soft check: check only words not in dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0PvbXVJMjjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check if word exists\n",
        "'word' in words.words()     # nltk\n",
        "d = enchant.Dict(\"en_US\")   # PyEnchant\n",
        "#---------------------------\n",
        "'''\n",
        "# dictionaries in PyEnchant can be installed with apt-get\n",
        "    - myspell-dictionary\n",
        "    - aspell-dictionary\n",
        "    - openoffice.org-dictionaries\n",
        "    - ispell-dictionary\n",
        "''';"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ4_5grLRcSf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "outputId": "15e6a549-a94b-41e8-dd05-179f90781470"
      },
      "source": [
        "# PyEnchant\n",
        "print(d.check('sciences'))\n",
        "print(d.check('siences'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TucJ4VkxSB8n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "a9621898-cb63-4124-ff1e-231cb602890d"
      },
      "source": [
        "# nltk\n",
        "print('sciences' in words.words())\n",
        "print('science' in words.words())\n",
        "print('siences' in words.words())\n",
        "print(len(words.words()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n",
            "False\n",
            "236736\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKp71IhaLIAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def soft_check(sentence):\n",
        "    tokenized = sentence.split()\n",
        "    for i, token in enumerate(tokenized):\n",
        "        #if not token in words.words():\n",
        "        if not d.check(token):\n",
        "            tokenized[i] = '[MASK]'\n",
        "    return tokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47v-79E_F-P5",
        "colab_type": "text"
      },
      "source": [
        "## 11.2 Back Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmF536YpGBZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Marian-NMT"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PtuXpG9wRimA"
      },
      "source": [
        "# End of the notebook"
      ]
    }
  ]
}