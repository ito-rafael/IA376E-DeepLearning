{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aula 7 - Complete Self-Attention with Good Practices (Rafael Ito).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OG5DT_dm6mk",
        "colab_type": "text"
      },
      "source": [
        "# Notebook de referência \n",
        "\n",
        "Usar as secções como guia.\n",
        "\n",
        "Nome: Rafael Ito"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od7iUgHy5SSi",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Neste colab iremos treinar um modelo para fazer análise de sentimento usando o dataset IMDB."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF94hPU1HWt9",
        "colab_type": "text"
      },
      "source": [
        "Installing packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44YWHZaqwDo4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q \\\n",
        "    numpy       \\\n",
        "    torch       \\\n",
        "    sklearn     \\\n",
        "    skorch      \\\n",
        "    matplotlib  \\\n",
        "    tqdm        \\\n",
        "    pytorch_lightning"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCtwg-IOtzHS",
        "colab_type": "text"
      },
      "source": [
        "Habilitamos o linting (avisa sobre erros de formatação no código)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8RKtorztzTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install --quiet flake8-nb pycodestyle_magic\n",
        "#%load_ext pycodestyle_magic\n",
        "#%flake8_on\n",
        "##%flake8_off"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls7N8NK8IEvG",
        "colab_type": "text"
      },
      "source": [
        "Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8KBwDxXLAhn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        },
        "outputId": "5327123b-3037-4d20-a464-6ab791db9940"
      },
      "source": [
        "#-------------------------------------------------\n",
        "# general\n",
        "#-------------------\n",
        "import numpy as np\n",
        "#import pandas as pd\n",
        "import pytorch_lightning as pl\n",
        "from multiprocessing import cpu_count\n",
        "#----------------------------\n",
        "import pdb\n",
        "# pdb.set_trace() # breakpoint\n",
        "#-------------------------------------------------\n",
        "# PyTorch\n",
        "#-------------------\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import Dataset\n",
        "from torchtext.vocab import GloVe\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Module\n",
        "from torch.nn import Linear\n",
        "from torch.nn import Dropout\n",
        "from torch.nn import LayerNorm\n",
        "from torch.nn import Embedding\n",
        "from torch.nn import Sequential\n",
        "#-------------------------------------------------\n",
        "# skorch\n",
        "#-------------------\n",
        "#from skorch import NeuralNetClassifier\n",
        "#-------------------------------------------------\n",
        "# scikit-learn\n",
        "#-------------------\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "#-------------------------------------------------\n",
        "# data visualization\n",
        "#-------------------\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#-------------------------------------------------\n",
        "# additional config\n",
        "#-------------------\n",
        "# random seed generator\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42);"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeaUTO_Zm-eE",
        "colab_type": "text"
      },
      "source": [
        "Descobrimos se há uma GPU disponível"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_o4uxnB9wSR",
        "colab_type": "code",
        "outputId": "e537313a-17d2-418c-df6a-bdba10b114cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        }
      },
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\" \n",
        "print(dev)\n",
        "device = torch.device(dev)\n",
        "#----------------------------\n",
        "# get GPU model used\n",
        "GPU_model = torch.cuda.get_device_name(0)\n",
        "print('GPU model:', GPU_model)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "GPU model: Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXFdJz2KVeQw",
        "colab_type": "text"
      },
      "source": [
        "## Preparando Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHMi_Kq65fPM",
        "colab_type": "text"
      },
      "source": [
        "Primeiro, fazemos download do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wbnfzst5O3k",
        "colab_type": "code",
        "outputId": "7d30b889-2641-47e1-a8e8-2745d4be594c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        }
      },
      "source": [
        "!wget -nc http://files.fast.ai/data/aclImdb.tgz \n",
        "!tar -xzf aclImdb.tgz"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File ‘aclImdb.tgz’ already there; not retrieving.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Giyi5Rv_NIm",
        "colab_type": "text"
      },
      "source": [
        "## Carregando o dataset\n",
        "\n",
        "Criaremos uma divisão de treino (80%) e dev (20%) artificialmente.\n",
        "\n",
        "Nota: Evitar de olhar ao máximo o dataset de teste para não ficar enviseado no que será testado. Em aplicações reais, o dataset de teste só estará disponível no futuro, ou seja, é quando o usuário começa a testar o seu produto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HIN_xLI_TuT",
        "colab_type": "code",
        "outputId": "0c8204fe-a638-454b-c0a9-042cf4951c57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "def load_texts(folder):\n",
        "    texts = []\n",
        "    for path in os.listdir(folder):\n",
        "        with open(os.path.join(folder, path)) as f:\n",
        "            texts.append(f.read())\n",
        "    return texts\n",
        "\n",
        "x_train_pos = load_texts('aclImdb/train/pos')\n",
        "x_train_neg = load_texts('aclImdb/train/neg')\n",
        "x_test_pos = load_texts('aclImdb/test/pos')\n",
        "x_test_neg = load_texts('aclImdb/test/neg')\n",
        "\n",
        "x_train = x_train_pos + x_train_neg\n",
        "x_test = x_test_pos + x_test_neg\n",
        "y_train = [True] * len(x_train_pos) + [False] * len(x_train_neg)\n",
        "y_test = [True] * len(x_test_pos) + [False] * len(x_test_neg)\n",
        "\n",
        "# Embaralhamos o treino para depois fazermos a divisão treino/dev.\n",
        "c = list(zip(x_train, y_train))\n",
        "random.shuffle(c)\n",
        "x_train, y_train = zip(*c)\n",
        "\n",
        "n_train = int(0.8 * len(x_train))\n",
        "\n",
        "x_dev = x_train[n_train:]\n",
        "y_dev = y_train[n_train:]\n",
        "x_train = x_train[:n_train]\n",
        "y_train = y_train[:n_train]\n",
        "\n",
        "print(len(x_train), 'amostras de treino.')\n",
        "print(len(x_dev), 'amostras de desenvolvimento.')\n",
        "print(len(x_test), 'amostras de teste.')\n",
        "\n",
        "print('3 primeiras amostras treino:')\n",
        "for x, y in zip(x_train[:3], y_train[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras treino:')\n",
        "for x, y in zip(x_train[-3:], y_train[-3:]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 primeiras amostras dev:')\n",
        "for x, y in zip(x_dev[:3], y_test[:3]):\n",
        "    print(y, x[:100])\n",
        "\n",
        "print('3 últimas amostras dev:')\n",
        "for x, y in zip(x_dev[-3:], y_dev[-3:]):\n",
        "    print(y, x[:100])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000 amostras de treino.\n",
            "5000 amostras de desenvolvimento.\n",
            "25000 amostras de teste.\n",
            "3 primeiras amostras treino:\n",
            "True Hey now, yours truly, TheatreX, found this while grubbing through videos at the flea market, in almo\n",
            "False \"That 'Malcom' show on FOX is really making a killing... can't we do our own version?\" I speculate a\n",
            "True This show was appreciated by critics and those who realized that any similarities between \"Pushing D\n",
            "3 últimas amostras treino:\n",
            "True Rawhide was a wonderful TV western series. Focusing on a band of trail drovers lead by the trail bos\n",
            "True I loved this film when I was little. Today at 17 it is one of my all time favorite animated films. B\n",
            "True I've read some terrible things about this film, so I was prepared for the worst. \"Confusing. Muddled\n",
            "3 primeiras amostras dev:\n",
            "True This is one of those movies that you and a bunch of friends sit around drinking beers, eating pizza,\n",
            "True Oh man. If you want to give your internal Crow T. Robot a real workout, this is the movie to pop int\n",
            "True This film is awful. Not offensive but extremely predictable. The movie follows the life of a small t\n",
            "3 últimas amostras dev:\n",
            "False (Rating: 21 by The Film Snob.) (See our blog What-To-See-Next for details on our rating system.)<br \n",
            "False I'm afraid that I didn't like this movie very much. Apart from a few saving graces, it's nothing to \n",
            "False IT was no sense and it was so awful... i think Hollywood have a lot of film like that... you don't h\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N570Kp3jJeFe",
        "colab_type": "text"
      },
      "source": [
        "## Download do word embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxflUUMBzk-k",
        "colab_type": "text"
      },
      "source": [
        "Lista dos modelos disponíveis: https://github.com/RaRe-Technologies/gensim-data#models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxYWSFfizpqJ",
        "colab_type": "code",
        "outputId": "2f24fe8f-b79d-4276-bc6f-e1b314436745",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "word2vec_model = api.load(\"glove-wiki-gigaword-300\")\n",
        "print('word2vec shape:', word2vec_model.vectors.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:summarizer.preprocessing.cleaner:'pattern' package not found; tag filters are not available for English\n",
            "INFO:gensim.models.utils_any2vec:loading projection weights from /root/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "INFO:gensim.models.utils_any2vec:loaded (400000, 300) matrix from /root/gensim-data/glove-wiki-gigaword-300/glove-wiki-gigaword-300.gz\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "word2vec shape: (400000, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xp2Fan8K04Y",
        "colab_type": "text"
      },
      "source": [
        "## Criando Vocabulário a partir do word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2VnCCqBJ6P6",
        "colab_type": "code",
        "outputId": "79962364-c7a9-4c14-c4b9-35ca1d0effc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "import itertools\n",
        "\n",
        "vocab = {word: index for index, word in enumerate(word2vec_model.index2word)}\n",
        "\n",
        "# Adicionando PAD token\n",
        "vocab['[PAD]'] = len(vocab)\n",
        "pad_vector = np.zeros((1, word2vec_model.vectors.shape[1]))\n",
        "embeddings = np.concatenate((word2vec_model.vectors, pad_vector), axis=0)\n",
        "# convert embeddings from numpy to pytorch float32\n",
        "embeddings = torch.from_numpy(embeddings)\n",
        "embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
        "                          \n",
        "print('Número de palavras no vocabulário:', len(vocab))\n",
        "print(f'20 tokens mais frequentes: {list(itertools.islice(vocab.keys(), 20))}')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  # This is added back by InteractiveShellApp.init_path()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Número de palavras no vocabulário: 400001\n",
            "20 tokens mais frequentes: ['the', ',', '.', 'of', 'to', 'and', 'in', 'a', '\"', \"'s\", 'for', '-', 'that', 'on', 'is', 'was', 'said', 'with', 'he', 'as']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpXMGPLHVUCS",
        "colab_type": "text"
      },
      "source": [
        "## Tokenizando o dataset e convertendo para índices (preferencialmente, usar o DataLoader)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMen-JFKLFCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab, seq_length=64):\n",
        "        self.texts = texts\n",
        "        self.labels = torch.tensor(labels, dtype=torch.int64)\n",
        "        self.vocab = vocab\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        words_idx = self.tokens_to_ids_batch(self.texts, self.vocab)\n",
        "        X_str, mask = self.truncate_and_pad(\n",
        "                            batch_word_ids=words_idx, \n",
        "                            pad_token_id=self.vocab['[PAD]'], \n",
        "                            seq_length=self.seq_length)\n",
        "        self.X = torch.tensor(X_str, dtype=torch.int64)\n",
        "        self.mask = torch.tensor(mask, dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.X[index], self.labels[index], self.mask[index])\n",
        "                 \n",
        "    def tokenize(self, texts):\n",
        "        for char in ['\"', '\\'', '.', ',', ':', '-', '?', '!']:\n",
        "            texts = texts.replace(char, ' ')\n",
        "        return texts.lower().split()\n",
        "\n",
        "    def tokens_to_ids(self, tokens, vocab):\n",
        "        return [vocab[token] for token in tokens if token in vocab]\n",
        "\n",
        "    def tokens_to_ids_batch(self, textss, vocab):\n",
        "        return [self.tokens_to_ids(self.tokenize(texts), vocab) for texts in textss]\n",
        "\n",
        "    def truncate_and_pad(self, batch_word_ids, pad_token_id, seq_length):\n",
        "        batch_word_ids = [word_ids[:seq_length] for word_ids in batch_word_ids]\n",
        "        mask = [\n",
        "            [1] * len(word_ids) + [0] * (seq_length - len(word_ids))\n",
        "            for word_ids in  batch_word_ids]\n",
        "        batch_word_ids = [\n",
        "            word_ids + [pad_token_id] * (seq_length - len(word_ids))\n",
        "            for word_ids in batch_word_ids]\n",
        "        return batch_word_ids, mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a8kAF4zCAol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cloyt0tIwIiD",
        "colab_type": "text"
      },
      "source": [
        "## Inicializando e testando o DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoKiQXCvwGrP",
        "colab_type": "code",
        "outputId": "357df590-4fb1-4ac6-9808-a3ab576cc822",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "texts = ['we like pizza', 'he does not like apples']\n",
        "labels = [0, 1]\n",
        "mydataset_debug = MyDataset(\n",
        "    texts=texts,\n",
        "    labels=labels,\n",
        "    vocab=vocab,\n",
        "    #pad_token_id=vocab['[PAD]'],\n",
        "    seq_length=100)\n",
        "\n",
        "L_DEBUG = 100\n",
        "B_DEBUG = 10\n",
        "dataloader_debug = DataLoader(mydataset_debug, batch_size=10, shuffle=True, \n",
        "                              num_workers=0)\n",
        "\n",
        "batch_token_ids, batch_labels, batch_mask = next(iter(dataloader_debug))\n",
        "print('batch_token_ids', batch_token_ids)\n",
        "print('batch_labels', batch_labels)\n",
        "print('batch_token_ids.shape:', batch_token_ids.shape)\n",
        "print('batch_labels.shape:', batch_labels.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch_token_ids tensor([[    53,    117,   9388, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000],\n",
            "        [    18,    260,     36,    117,  13134, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
            "         400000]])\n",
            "batch_labels tensor([0, 1])\n",
            "batch_token_ids.shape: torch.Size([2, 100])\n",
            "batch_labels.shape: torch.Size([2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFyvSJCVpyTp",
        "colab_type": "text"
      },
      "source": [
        "dimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b43wrZOmnXFU",
        "colab_type": "code",
        "outputId": "10be1af0-44e3-4c6e-d2f6-f741daabde5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "V = len(vocab)\n",
        "D = word2vec_model.vector_size\n",
        "L = 200\n",
        "H = 6\n",
        "B = BATCH_SIZE\n",
        "#---------------------------\n",
        "print('V =', V)     # vocab size\n",
        "print('D =', D)     # embedding dim\n",
        "print('H =', H)     # length of sequence\n",
        "print('L =', L)     # number of heads\n",
        "print('B =', B)     # batch size"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "V = 400001\n",
            "D = 300\n",
            "H = 6\n",
            "L = 200\n",
            "B = 128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo8oCnIGXntL",
        "colab_type": "text"
      },
      "source": [
        "## Definindo a Rede Neural"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tviAwxUpuajT",
        "colab_type": "text"
      },
      "source": [
        "Multi-Head Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9tq3Onsm_0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHead(torch.nn.Module):\n",
        "    def __init__(self, L, D, H):\n",
        "        super(MultiHead, self).__init__()\n",
        "        self.L = L  # length of sequence\n",
        "        self.D = D  # embedding dim\n",
        "        self.H = H  # number of heads\n",
        "        #---------------------------\n",
        "        self.W_q = Linear(self.D, self.D, bias=False)\n",
        "        self.W_k = Linear(self.D, self.D, bias=False)\n",
        "        self.W_v = Linear(self.D, self.D, bias=False)\n",
        "        self.W_o = Linear(self.D, self.D, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # multi-head (linear projections)\n",
        "        q = self.W_q(x).view(-1, self.L, self.H, int(self.D/self.H))\n",
        "        q = self.W_q(x).view(-1, self.L, self.H, int(self.D/self.H))\n",
        "        k = self.W_k(x).view(-1, self.L, self.H, int(self.D/self.H))\n",
        "        v = self.W_v(x).view(-1, self.L, self.H, int(self.D/self.H))\n",
        "        # transpose to (H, L, D/H)\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "        #-------------------\n",
        "        # calculate self-attention\n",
        "        k = k.transpose(2,3)\n",
        "        self_attention = ((q @ k) / torch.sqrt(torch.tensor(self.D, dtype=torch.float32))) @ v\n",
        "        new_x = F.softmax(self_attention)\n",
        "        #-------------------\n",
        "        new_x = new_x.transpose(1, 2).contiguous()\n",
        "        new_x = new_x.view(-1, self.L, self.D)\n",
        "        #-------------------\n",
        "        # output linear projections\n",
        "        return self.W_o(new_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRjc9msmujBy",
        "colab_type": "text"
      },
      "source": [
        "Feed Forward Network Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAaW2SdhrNV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP2Layer(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, bias=True):\n",
        "        super(MLP2Layer, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.bias = bias\n",
        "        self.dropout = Dropout(0.1)\n",
        "        #---------------------------\n",
        "        self.hidden = Linear(in_features=self.input_size, out_features=self.hidden_size, bias=self.bias)\n",
        "        self.output = Linear(in_features=self.hidden_size, out_features=self.output_size, bias=self.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.hidden(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCJVqqltu6cF",
        "colab_type": "text"
      },
      "source": [
        "Network model definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OsQ-0ceVQ78U",
        "colab": {}
      },
      "source": [
        "'''\n",
        "V: vocabulary size\n",
        "D: dimension of embeddings\n",
        "H: number of heads in multi-head\n",
        "L: legth of the sequence (number of words)\n",
        "B: batch size\n",
        "'''\n",
        "class SelfAttentionNN(Module):\n",
        "    def __init__(self, D, L, H, B, idx2vec, device):\n",
        "        super(SelfAttentionNN, self).__init__()\n",
        "        self.L = L  # length of sequence\n",
        "        self.D = D  # embedding dim\n",
        "        self.H = H  # number of heads\n",
        "        self.B = B  # batch size\n",
        "        self.device = device\n",
        "        self.idx2vec = idx2vec.to(self.device)\n",
        "        #-------------------------------------------------\n",
        "        # embeddings\n",
        "        self.positions = torch.arange(self.L).to(self.device)\n",
        "        self.pos_emb = Embedding(num_embeddings=self.L, embedding_dim=self.D)\n",
        "        #-------------------\n",
        "        # multi-head attention\n",
        "        self.multihead = MultiHead(self.L, self.D, self.H)\n",
        "        self.norm1 = LayerNorm(self.D)\n",
        "        #-------------------\n",
        "        # feed-forward network\n",
        "        self.ffn = MLP2Layer(self.D, self.D, self.D, bias=False)\n",
        "        self.norm2 = LayerNorm(self.D)\n",
        "        #-------------------\n",
        "        # MLP classifier layer\n",
        "        self.mlp = MLP2Layer(self.D, self.D, 2)\n",
        "        #-------------------------------------------------\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        #-------------------\n",
        "        # get embeddings\n",
        "        x = self.idx2vec[x]\n",
        "#        pdb.set_trace() # breakpoint\n",
        "        # sum with positional embeddings\n",
        "        x = x + self.pos_emb(self.positions)\n",
        "        #-------------------\n",
        "        # multi-head attention\n",
        "        residual = x.clone()\n",
        "        x = self.multihead(x)\n",
        "        # add & norm\n",
        "        x = x + residual\n",
        "        x = self.norm1(x)\n",
        "        #-------------------\n",
        "        # feed forward\n",
        "        residual = x.clone()\n",
        "        x = self.ffn(x)\n",
        "        # add & norm\n",
        "        x = x + residual\n",
        "        x = self.norm1(x)\n",
        "        #-------------------\n",
        "        # masked mean\n",
        "        x = x * mask.reshape(-1, self.L, 1)\n",
        "        #seq_len = torch.nonzero(mask).size(0)\n",
        "        seq_len = self.L - (mask == 0).sum(dim=1)\n",
        "        seq_len = seq_len.reshape(-1,1)\n",
        "        x = (torch.sum(x, dim=1) / seq_len)\n",
        "        #-------------------\n",
        "        # final mlp\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjZZhjixzXGN",
        "colab_type": "text"
      },
      "source": [
        "## Número de parâmetros do modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaXoBX2Qza8I",
        "colab_type": "code",
        "outputId": "8a36015f-9386-4329-e26c-ae8b03e32b54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "model = SelfAttentionNN(D, L, H, B, embeddings, device)\n",
        "sum([torch.tensor(x.size()).prod() for x in model.parameters() if x.requires_grad]) # trainable parameters"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(692102)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDcbeDvO71VL",
        "colab_type": "text"
      },
      "source": [
        "## Testando o modelo com um batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2f6OPLGQcBW",
        "colab_type": "code",
        "outputId": "69446894-1dcc-4d99-d2c6-97d20b228f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "model = SelfAttentionNN(D, L_DEBUG, H, B_DEBUG, embeddings, device)\n",
        "model = model.to(device)\n",
        "print('Saída do modelo:')\n",
        "batch_token_ids, batch_mask = batch_token_ids.to(device), batch_mask.to(device)\n",
        "print(model(batch_token_ids, batch_mask))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saída do modelo:\n",
            "tensor([[ 0.1734,  0.0273],\n",
            "        [-0.0888,  0.1318]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCTbm13hQJB7",
        "colab_type": "text"
      },
      "source": [
        "Definindo as funções de treino e validação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zByG_J6XOhGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(dataloader, model, optimizer, criterion):\n",
        "    \n",
        "    model.train()\n",
        "    loss_sum = 0.0\n",
        "    for iteration, (X_batch, y_batch, mask_batch) in enumerate(dataloader):\n",
        "        \n",
        "        X_batch = X_batch.to(device)\n",
        "        y_batch = y_batch.to(device)\n",
        "        mask_batch = mask_batch.to(device)\n",
        "\n",
        "        # Precisamos zerar os gradientes acumulados na iteração anterior\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        #logits = model(token_ids=X_batch)\n",
        "        logits = model(X_batch, mask_batch)\n",
        "        \n",
        "        loss = criterion(logits, y_batch)\n",
        "        loss_sum += loss\n",
        "        # Aqui que rodamos o backpropagation para calcular os gradientes.\n",
        "        loss.backward()\n",
        "\n",
        "        # Aqui os pesos da rede são ajustados com base nos gradientes calculados\n",
        "        # acima e o optimizador atualiza suas variáveis internas (taxa de\n",
        "        # aprendizado, decaimento, etc).\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss = loss_sum / len(dataloader)\n",
        "    return average_loss.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MqiiLHV9PQ1V",
        "colab": {}
      },
      "source": [
        "def evaluate(dataloader, model):\n",
        "    matches = 0.0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch, mask_batch in dataloader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            mask_batch = mask_batch.to(device)\n",
        "\n",
        "            #logits = model(token_ids=X_batch)\n",
        "            logits = model(X_batch, mask_batch)\n",
        "            class_predictons = logits.argmax(dim=1)\n",
        "            \n",
        "            matches += (class_predictons == y_batch).sum()\n",
        "\n",
        "    accuracy = matches / len(dataloader.dataset)\n",
        "    return accuracy.item()        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS2bjddQu87g",
        "colab_type": "text"
      },
      "source": [
        "## Overfit em um batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEr29GIhBNKs",
        "colab_type": "text"
      },
      "source": [
        "Antes de treinar o modelo no dataset todo, faremos overfit do modelo em um único minibatch de treino para verificar se loss vai para próximo de 0. Isso serve para depurar se a implementação do modelo está correta.\n",
        "\n",
        "Podemos também medir se a acurácia neste minibatch chega perto de 100%. Isso serve para depurar se nossa função que mede a acurácia está correta.\n",
        "\n",
        "Nota: se treinarmos por muitas épocas (ex: 500) é possivel que a loss vá para zero mesmo com bugs na implementação. O ideal é que a loss chege próxima a zero antes de 100 épocas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92RcHIEONdzB",
        "colab_type": "code",
        "outputId": "7f8091a8-2c20-4b4e-ccf8-c1cf8e543536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "N_EPOCHS = 100\n",
        "\n",
        "model = SelfAttentionNN(D, L, H, B, embeddings, device)\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "mydataset_debug = MyDataset(\n",
        "    texts=x_train[:10],\n",
        "    labels=y_train[:10],\n",
        "    vocab=vocab,\n",
        "    #pad_token_id=vocab['[PAD]'],\n",
        "    seq_length=200)\n",
        "\n",
        "dataloader_debug = DataLoader(mydataset_debug, batch_size=10, shuffle=True, \n",
        "                              num_workers=0)\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    average_loss = train(dataloader=dataloader_debug,\n",
        "                         model=model,\n",
        "                         optimizer=optimizer,\n",
        "                         criterion=criterion)\n",
        "\n",
        "    train_accuracy = evaluate(dataloader=dataloader_debug, model=model)\n",
        "    print(f'epoch: {epoch} '\n",
        "          f'average training loss: {average_loss:.3f} '\n",
        "          f'training accuracy: {train_accuracy:.3f}')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 average training loss: 0.701 training accuracy: 0.600\n",
            "epoch: 1 average training loss: 0.677 training accuracy: 0.600\n",
            "epoch: 2 average training loss: 0.662 training accuracy: 0.600\n",
            "epoch: 3 average training loss: 0.655 training accuracy: 0.600\n",
            "epoch: 4 average training loss: 0.640 training accuracy: 0.600\n",
            "epoch: 5 average training loss: 0.618 training accuracy: 0.600\n",
            "epoch: 6 average training loss: 0.609 training accuracy: 0.600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 7 average training loss: 0.560 training accuracy: 0.900\n",
            "epoch: 8 average training loss: 0.550 training accuracy: 0.700\n",
            "epoch: 9 average training loss: 0.470 training accuracy: 1.000\n",
            "epoch: 10 average training loss: 0.393 training accuracy: 1.000\n",
            "epoch: 11 average training loss: 0.281 training accuracy: 1.000\n",
            "epoch: 12 average training loss: 0.196 training accuracy: 1.000\n",
            "epoch: 13 average training loss: 0.112 training accuracy: 1.000\n",
            "epoch: 14 average training loss: 0.045 training accuracy: 1.000\n",
            "epoch: 15 average training loss: 0.017 training accuracy: 1.000\n",
            "epoch: 16 average training loss: 0.006 training accuracy: 1.000\n",
            "epoch: 17 average training loss: 0.003 training accuracy: 1.000\n",
            "epoch: 18 average training loss: 0.001 training accuracy: 1.000\n",
            "epoch: 19 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 20 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 21 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 22 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 23 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 24 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 25 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 26 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 27 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 28 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 29 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 30 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 31 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 32 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 33 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 34 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 35 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 36 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 37 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 38 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 39 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 40 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 41 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 42 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 43 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 44 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 45 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 46 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 47 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 48 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 49 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 50 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 51 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 52 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 53 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 54 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 55 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 56 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 57 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 58 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 59 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 60 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 61 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 62 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 63 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 64 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 65 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 66 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 67 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 68 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 69 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 70 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 71 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 72 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 73 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 74 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 75 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 76 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 77 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 78 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 79 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 80 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 81 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 82 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 83 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 84 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 85 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 86 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 87 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 88 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 89 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 90 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 91 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 92 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 93 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 94 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 95 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 96 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 97 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 98 average training loss: 0.000 training accuracy: 1.000\n",
            "epoch: 99 average training loss: 0.000 training accuracy: 1.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKEddD8nvSpt",
        "colab_type": "text"
      },
      "source": [
        "## Treinamento e Validação no dataset todo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V9RzCYcavfhR",
        "outputId": "49f22071-a985-41bc-8b8e-208b39b6bc27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        }
      },
      "source": [
        "import time\n",
        "N_EPOCHS = 8\n",
        "\n",
        "model = SelfAttentionNN(D, L, H, B, embeddings, device)\n",
        "model.to(device)\n",
        "#optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "ds_train = MyDataset(x_train, y_train, vocab, seq_length=200)\n",
        "\n",
        "dl_train = torch.utils.data.DataLoader(\n",
        "            dataset=ds_train, \n",
        "            drop_last = False,\n",
        "            shuffle = True,\n",
        "            batch_size = BATCH_SIZE)\n",
        "\n",
        "ds_valid = MyDataset(x_dev, y_dev, vocab, seq_length=200)\n",
        "\n",
        "dl_valid =  torch.utils.data.DataLoader(\n",
        "            dataset = ds_valid,\n",
        "            drop_last = False,\n",
        "            shuffle = False,\n",
        "            batch_size = BATCH_SIZE)\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(dataloader=dl_train,\n",
        "                        model=model,\n",
        "                        optimizer=optimizer,\n",
        "                        criterion=criterion)\n",
        "    train_time = time.time() - start_time\n",
        "\n",
        "    start_time = time.time()\n",
        "    train_accuracy = evaluate(dataloader=dl_train, model=model)\n",
        "    dev_accuracy = evaluate(dataloader=dl_valid, model=model)\n",
        "    eval_time = time.time() - start_time\n",
        "\n",
        "    print(f'epoch: {epoch} '\n",
        "          f'training loss: {train_loss:.3f} '\n",
        "          f'training accuracy: {train_accuracy:.3f} '\n",
        "          f'dev accuracy: {dev_accuracy:.3f}')\n",
        "\n",
        "    train_examples_per_sec = len(dl_train.dataset) / train_time\n",
        "    eval_examples_per_sec = (\n",
        "        len(dl_train.dataset) + len(dl_valid.dataset))/ eval_time\n",
        "    print(f'total training time: {train_time:.3f} '\n",
        "          f'total eval time: {eval_time:.3f}')\n",
        "    print(f'training examples/sec: {train_examples_per_sec:.2f} '\n",
        "          f'eval examples/sec: {eval_examples_per_sec:.2f}')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 0 training loss: 0.596 training accuracy: 0.752 dev accuracy: 0.755\n",
            "total training time: 4.923 total eval time: 2.232\n",
            "training examples/sec: 4062.55 eval examples/sec: 11199.71\n",
            "epoch: 1 training loss: 0.453 training accuracy: 0.785 dev accuracy: 0.765\n",
            "total training time: 4.924 total eval time: 2.235\n",
            "training examples/sec: 4062.15 eval examples/sec: 11184.88\n",
            "epoch: 2 training loss: 0.410 training accuracy: 0.833 dev accuracy: 0.823\n",
            "total training time: 4.927 total eval time: 2.242\n",
            "training examples/sec: 4059.45 eval examples/sec: 11149.18\n",
            "epoch: 3 training loss: 0.386 training accuracy: 0.849 dev accuracy: 0.832\n",
            "total training time: 4.932 total eval time: 2.245\n",
            "training examples/sec: 4054.95 eval examples/sec: 11134.21\n",
            "epoch: 4 training loss: 0.378 training accuracy: 0.836 dev accuracy: 0.820\n",
            "total training time: 4.934 total eval time: 2.249\n",
            "training examples/sec: 4053.75 eval examples/sec: 11118.05\n",
            "epoch: 5 training loss: 0.377 training accuracy: 0.821 dev accuracy: 0.795\n",
            "total training time: 4.942 total eval time: 2.254\n",
            "training examples/sec: 4047.32 eval examples/sec: 11091.02\n",
            "epoch: 6 training loss: 0.361 training accuracy: 0.861 dev accuracy: 0.837\n",
            "total training time: 4.946 total eval time: 2.260\n",
            "training examples/sec: 4043.31 eval examples/sec: 11059.89\n",
            "epoch: 7 training loss: 0.345 training accuracy: 0.855 dev accuracy: 0.825\n",
            "total training time: 4.952 total eval time: 2.270\n",
            "training examples/sec: 4038.93 eval examples/sec: 11015.33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjN_-W4WZ67q",
        "colab_type": "text"
      },
      "source": [
        "## Após treinado, avaliamos o modelo no dataset de test.\n",
        "\n",
        "É importante que essa avaliação seja feita poucas vezes para evitar o overfit no dataset de teste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk6myBCOaajB",
        "colab_type": "code",
        "outputId": "ec6bd5e7-3f5f-4dd3-e0ca-0c9204fb08c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 78
        }
      },
      "source": [
        "ds_test  = MyDataset(x_test, y_test,  vocab, seq_length=200)\n",
        "\n",
        "dl_test =  torch.utils.data.DataLoader(\n",
        "            dataset = ds_test,\n",
        "            drop_last = False,\n",
        "            shuffle = False,\n",
        "            batch_size = BATCH_SIZE)\n",
        "\n",
        "test_accuracy = evaluate(dataloader=dl_test, model=model)\n",
        "print(f'test accuracy: {test_accuracy:.3f}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "test accuracy: 0.821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTZrQ_FhLTX6",
        "colab_type": "text"
      },
      "source": [
        "## End of Notebook"
      ]
    }
  ]
}